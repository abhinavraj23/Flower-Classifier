{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_challenge.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "mTKHYzIbsaS4",
        "colab_type": "code",
        "outputId": "fff34cd0-8d29-4119-ed9c-3bb744a768c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x57f8e000 @  0x7f97b194d2a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q4r5lG2Wsn3L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8GGe0Os-_s0D",
        "colab_type": "code",
        "outputId": "11344a86-2061-410a-9b94-e9e435935cad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4.1\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7sew3NXN_wlY",
        "colab_type": "code",
        "outputId": "6eb2d50f-5c8b-4beb-aa91-a1a057cadfab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Da2jv9L1__l1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Imports here\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision import datasets\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torchvision import models\n",
        "\n",
        "import cv2\n",
        "import helper\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CT3x8mIdATlX",
        "colab_type": "code",
        "outputId": "78aeed03-929a-4f8a-ed7e-79f52de46459",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install Pillow==4.0.0\n",
        "!pip install PIL\n",
        "!pip install image\n",
        "\n",
        "# convert data to a normalized torch.FloatTensor\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "    ])\n",
        "\n",
        "valid_transforms = transforms.Compose([\n",
        "    transforms.Resize(255),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "    ])\n",
        "\n",
        "# TODO: Load the datasets with ImageFolder\n",
        "train_data = datasets.ImageFolder('drive/My Drive/AI/flower_data/train', transform=train_transforms)\n",
        "valid_data = datasets.ImageFolder('drive/My Drive/AI/flower_data/valid', transform=valid_transforms)\n",
        "\n",
        "# TODO: Using the image datasets and the trainforms, define the dataloaders\n",
        "trainLoader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True) \n",
        "validLoader = torch.utils.data.DataLoader(valid_data, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Pillow==4.0.0\n",
            "  Using cached https://files.pythonhosted.org/packages/37/e8/b3fbf87b0188d22246678f8cd61e23e31caa1769ebc06f1664e2e5fe8a17/Pillow-4.0.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow==4.0.0) (0.46)\n",
            "\u001b[31mtorchvision 0.2.1 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Pillow\n",
            "  Found existing installation: Pillow 5.3.0\n",
            "    Uninstalling Pillow-5.3.0:\n",
            "      Successfully uninstalled Pillow-5.3.0\n",
            "Successfully installed Pillow-4.0.0\n",
            "Collecting PIL\n",
            "\u001b[31m  Could not find a version that satisfies the requirement PIL (from versions: )\u001b[0m\n",
            "\u001b[31mNo matching distribution found for PIL\u001b[0m\n",
            "Requirement already satisfied: image in /usr/local/lib/python3.6/dist-packages (1.5.27)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from image) (4.0.0)\n",
            "Requirement already satisfied: django in /usr/local/lib/python3.6/dist-packages (from image) (2.1.4)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->image) (0.46)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from django->image) (2018.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xVVZ4pMmksfY",
        "colab_type": "code",
        "outputId": "0599a615-2a33-4527-a080-17c345fed6dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2584
        }
      },
      "cell_type": "code",
      "source": [
        "images,labels = next(iter(trainLoader))\n",
        "print(images)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[-0.7479, -0.8849, -0.9705,  ...,  0.6392,  0.5878,  0.7591],\n",
            "          [-0.7308, -0.8849, -0.9705,  ...,  0.7077,  0.6221,  0.5536],\n",
            "          [-0.7308, -0.9020, -1.0048,  ...,  0.7077,  0.6906,  0.6221],\n",
            "          ...,\n",
            "          [-2.1179, -2.1179, -2.1179,  ..., -1.6384, -1.6213, -1.6042],\n",
            "          [-2.1179, -2.1179, -2.1179,  ..., -1.6384, -1.6213, -1.6042],\n",
            "          [-2.1179, -2.1179, -2.1179,  ..., -1.6384, -1.6213, -1.6213]],\n",
            "\n",
            "         [[-1.0553, -1.2129, -1.3004,  ..., -0.4601, -0.4951, -0.2850],\n",
            "          [-1.0378, -1.2129, -1.3004,  ..., -0.4951, -0.5476, -0.5651],\n",
            "          [-1.0378, -1.2304, -1.3354,  ..., -0.4951, -0.5126, -0.5476],\n",
            "          ...,\n",
            "          [-2.0357, -2.0357, -2.0357,  ..., -1.5630, -1.5455, -1.5280],\n",
            "          [-2.0357, -2.0357, -2.0357,  ..., -1.5630, -1.5455, -1.5280],\n",
            "          [-2.0357, -2.0357, -2.0357,  ..., -1.5630, -1.5455, -1.5455]],\n",
            "\n",
            "         [[-0.8458, -0.9678, -1.1247,  ...,  0.9145,  0.8797,  1.0539],\n",
            "          [-0.8284, -0.9330, -1.1247,  ...,  0.9145,  0.8448,  0.8099],\n",
            "          [-0.8284, -0.9678, -1.0550,  ...,  0.9145,  0.8971,  0.8448],\n",
            "          ...,\n",
            "          [-1.8044, -1.8044, -1.8044,  ..., -1.6476, -1.6302, -1.6127],\n",
            "          [-1.8044, -1.8044, -1.8044,  ..., -1.6476, -1.6302, -1.6127],\n",
            "          [-1.8044, -1.8044, -1.8044,  ..., -1.6476, -1.6302, -1.6302]]],\n",
            "\n",
            "\n",
            "        [[[-2.0665, -2.0665, -2.0665,  ..., -2.0837, -2.0665, -2.0494],\n",
            "          [-2.0665, -2.0665, -2.0665,  ..., -2.1008, -2.0837, -2.0665],\n",
            "          [-2.0665, -2.0665, -2.0665,  ..., -2.1179, -2.1008, -2.1008],\n",
            "          ...,\n",
            "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
            "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
            "          [-2.1179, -2.1179, -2.1179,  ..., -2.1008, -2.1008, -2.1008]],\n",
            "\n",
            "         [[-1.9832, -1.9832, -1.9832,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          [-1.9832, -1.9832, -1.9832,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          [-1.9832, -1.9832, -1.9832,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          ...,\n",
            "          [-2.0357, -2.0357, -2.0357,  ..., -2.0182, -2.0182, -2.0182],\n",
            "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
            "\n",
            "         [[-1.7522, -1.7522, -1.7522,  ..., -1.7522, -1.7522, -1.7347],\n",
            "          [-1.7522, -1.7522, -1.7522,  ..., -1.7696, -1.7696, -1.7696],\n",
            "          [-1.7522, -1.7522, -1.7522,  ..., -1.8044, -1.7870, -1.8044],\n",
            "          ...,\n",
            "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
            "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
            "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
            "\n",
            "\n",
            "        [[[-1.6727, -1.6213, -1.6213,  ..., -2.1179, -2.1179, -2.1179],\n",
            "          [-1.6727, -1.6042, -1.5528,  ..., -2.1179, -2.1179, -2.1179],\n",
            "          [-1.6555, -1.5870, -1.4672,  ..., -2.1179, -2.1179, -2.1179],\n",
            "          ...,\n",
            "          [-1.0562, -1.1247, -1.1418,  ..., -2.1179, -2.1179, -2.1179],\n",
            "          [-1.0904, -1.1075, -1.1418,  ..., -2.1179, -2.1179, -2.1179],\n",
            "          [-1.0733, -1.0904, -1.1247,  ..., -2.1179, -2.1179, -2.1179]],\n",
            "\n",
            "         [[-1.1078, -1.0903, -1.0903,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          [-1.1078, -1.0553, -0.9853,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          [-1.0903, -1.0203, -0.8803,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          ...,\n",
            "          [-0.2150, -0.2325, -0.2850,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          [-0.1625, -0.1975, -0.2500,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          [-0.1450, -0.1800, -0.2325,  ..., -2.0357, -2.0357, -2.0357]],\n",
            "\n",
            "         [[-1.5953, -1.5953, -1.5953,  ..., -1.8044, -1.8044, -1.8044],\n",
            "          [-1.5953, -1.5779, -1.5430,  ..., -1.8044, -1.8044, -1.8044],\n",
            "          [-1.6302, -1.5953, -1.4907,  ..., -1.8044, -1.8044, -1.8044],\n",
            "          ...,\n",
            "          [-1.1770, -1.2293, -1.2293,  ..., -1.8044, -1.8044, -1.8044],\n",
            "          [-1.1944, -1.2119, -1.2119,  ..., -1.8044, -1.8044, -1.8044],\n",
            "          [-1.1770, -1.2119, -1.2119,  ..., -1.8044, -1.8044, -1.8044]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-2.0837, -2.0837, -2.0837,  ..., -2.1179, -2.1179, -2.1179],\n",
            "          [-2.0837, -2.0837, -2.0837,  ..., -2.1179, -2.1179, -2.1179],\n",
            "          [-2.0837, -2.0837, -2.0837,  ..., -2.1179, -2.1179, -2.1179],\n",
            "          ...,\n",
            "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
            "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
            "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
            "\n",
            "         [[-2.0007, -2.0007, -2.0007,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          [-2.0007, -2.0007, -2.0007,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          [-2.0007, -2.0007, -2.0007,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          ...,\n",
            "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
            "\n",
            "         [[-1.7696, -1.7696, -1.7696,  ..., -1.8044, -1.8044, -1.8044],\n",
            "          [-1.7696, -1.7696, -1.7696,  ..., -1.8044, -1.8044, -1.8044],\n",
            "          [-1.7696, -1.7696, -1.7696,  ..., -1.8044, -1.8044, -1.8044],\n",
            "          ...,\n",
            "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
            "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
            "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
            "\n",
            "\n",
            "        [[[ 2.2318,  2.2318,  2.2318,  ...,  0.5364,  0.5022,  0.4679],\n",
            "          [ 2.1975,  2.2489,  2.2489,  ...,  0.5193,  0.4337,  0.4337],\n",
            "          [ 2.1462,  2.1975,  2.2318,  ...,  0.4679,  0.4337,  0.4337],\n",
            "          ...,\n",
            "          [-1.5699, -1.6042, -1.5870,  ...,  1.2043,  1.1872,  1.1872],\n",
            "          [-1.5357, -1.5528, -1.5699,  ...,  1.2557,  1.2385,  1.2214],\n",
            "          [-1.5699, -1.5699, -1.5699,  ...,  1.2899,  1.2728,  1.2728]],\n",
            "\n",
            "         [[ 2.1835,  2.2010,  2.2010,  ...,  0.6604,  0.6779,  0.6604],\n",
            "          [ 2.1660,  2.2360,  2.2185,  ...,  0.6254,  0.5903,  0.6429],\n",
            "          [ 2.0959,  2.1835,  2.2535,  ...,  0.5553,  0.5203,  0.5728],\n",
            "          ...,\n",
            "          [-1.4930, -1.4930, -1.4755,  ...,  1.1506,  1.1155,  1.1155],\n",
            "          [-1.4930, -1.4930, -1.4755,  ...,  1.1856,  1.1681,  1.1506],\n",
            "          [-1.5280, -1.5280, -1.4930,  ...,  1.2206,  1.2031,  1.1856]],\n",
            "\n",
            "         [[ 2.6400,  2.6400,  2.6400,  ...,  0.4091,  0.3916,  0.3916],\n",
            "          [ 2.6400,  2.6400,  2.6400,  ...,  0.2522,  0.2522,  0.2348],\n",
            "          [ 2.6400,  2.6226,  2.6400,  ...,  0.1476,  0.1476,  0.1825],\n",
            "          ...,\n",
            "          [-1.3687, -1.3687, -1.3513,  ...,  1.0365,  1.0017,  1.0017],\n",
            "          [-1.3861, -1.4036, -1.3687,  ...,  1.1062,  1.0539,  1.0365],\n",
            "          [-1.4210, -1.4210, -1.3861,  ...,  1.1759,  1.1411,  1.0888]]],\n",
            "\n",
            "\n",
            "        [[[ 2.2318,  2.2489,  2.2318,  ...,  2.0605, -1.2274, -2.1179],\n",
            "          [ 2.2147,  2.2489,  2.2318,  ...,  2.1290, -0.5938, -2.1179],\n",
            "          [ 2.1975,  2.2489,  2.2489,  ...,  2.2489,  0.5878, -2.1179],\n",
            "          ...,\n",
            "          [-0.9363, -0.9534, -0.9705,  ..., -2.1179, -2.1179, -2.1179],\n",
            "          [-0.9363, -0.9363, -0.9534,  ..., -2.1179, -2.1179, -2.1179],\n",
            "          [-0.9363, -0.9363, -0.9534,  ..., -2.1179, -2.1179, -2.1179]],\n",
            "\n",
            "         [[ 2.4111,  2.3761,  2.4111,  ...,  2.2360, -1.1253, -2.0357],\n",
            "          [ 2.4111,  2.3936,  2.4111,  ...,  2.3060, -0.4776, -2.0357],\n",
            "          [ 2.4286,  2.3936,  2.3936,  ...,  2.4286,  0.7304, -2.0357],\n",
            "          ...,\n",
            "          [-0.5301, -0.5476, -0.5651,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          [-0.5301, -0.5301, -0.5476,  ..., -2.0357, -2.0357, -2.0357],\n",
            "          [-0.5301, -0.5301, -0.5476,  ..., -2.0357, -2.0357, -2.0357]],\n",
            "\n",
            "         [[ 2.5529,  2.6226,  2.5180,  ...,  2.4483, -0.8981, -1.8044],\n",
            "          [ 2.5354,  2.6226,  2.5180,  ...,  2.5180, -0.2532, -1.8044],\n",
            "          [ 2.5006,  2.6226,  2.5354,  ...,  2.6400,  0.9494, -1.8044],\n",
            "          ...,\n",
            "          [-0.4450, -0.4624, -0.4798,  ..., -1.8044, -1.8044, -1.8044],\n",
            "          [-0.4450, -0.4450, -0.4624,  ..., -1.8044, -1.8044, -1.8044],\n",
            "          [-0.4450, -0.4450, -0.4624,  ..., -1.8044, -1.8044, -1.8044]]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S9oKegU7Eh92",
        "colab_type": "code",
        "outputId": "fe3d5df1-617c-455c-bca0-cb8a861c4dd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "model = models.densenet121(pretrained=True) # densenet201 initially\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
            "  nn.init.kaiming_normal(m.weight.data)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "hFJD2n74E4pg",
        "colab_type": "code",
        "outputId": "2be82c9f-b07c-446d-d1b7-49d08525729b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8653
        }
      },
      "cell_type": "code",
      "source": [
        "# Freeze parameters so we don't backprop through them\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "for param in model.parameters():\n",
        "  \n",
        "    param.requires_grad = False\n",
        "\n",
        "from collections import OrderedDict\n",
        "classifier = nn.Sequential(OrderedDict([\n",
        "                          ('fc1', nn.Linear(1024, 1000)), # Change depths for densenet201\n",
        "                          ('relu1', nn.ReLU()),\n",
        "                          ('dropout1', nn.Dropout(p=0.2)),\n",
        "                          ('fc2', nn.Linear(1000, 512)),\n",
        "                          ('relu2', nn.ReLU()),\n",
        "                          ('dropout2', nn.Dropout(p=0.3)),\n",
        "                          ('fc3', nn.Linear(512, 102))\n",
        "                          ]))\n",
        "\n",
        "# ('output', nn.LogSoftmax(dim=1)) use this only as last structure in the classifier when not using SGD as optimizer\n",
        "    \n",
        "model.classifier = classifier\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(model.classifier.parameters(), lr=0.01, momentum=0.2) # momentum for SGD\n",
        "\n",
        "# New variable\n",
        "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DenseNet(\n",
              "  (features): Sequential(\n",
              "    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu0): ReLU(inplace)\n",
              "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (denseblock1): _DenseBlock(\n",
              "      (denselayer1): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer2): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer3): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer4): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer5): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer6): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (transition1): _Transition(\n",
              "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace)\n",
              "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "    )\n",
              "    (denseblock2): _DenseBlock(\n",
              "      (denselayer1): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer2): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer3): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer4): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer5): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer6): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer7): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer8): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer9): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer10): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer11): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer12): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (transition2): _Transition(\n",
              "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace)\n",
              "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "    )\n",
              "    (denseblock3): _DenseBlock(\n",
              "      (denselayer1): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer2): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer3): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer4): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer5): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer6): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer7): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer8): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer9): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer10): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer11): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer12): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer13): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer14): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer15): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer16): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer17): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer18): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer19): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer20): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer21): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer22): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer23): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer24): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (transition3): _Transition(\n",
              "      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace)\n",
              "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "    )\n",
              "    (denseblock4): _DenseBlock(\n",
              "      (denselayer1): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer2): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer3): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer4): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer5): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer6): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer7): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer8): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer9): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer10): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer11): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer12): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer13): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer14): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer15): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer16): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace)\n",
              "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (fc1): Linear(in_features=1024, out_features=1000, bias=True)\n",
              "    (relu1): ReLU()\n",
              "    (dropout1): Dropout(p=0.2)\n",
              "    (fc2): Linear(in_features=1000, out_features=512, bias=True)\n",
              "    (relu2): ReLU()\n",
              "    (dropout2): Dropout(p=0.3)\n",
              "    (fc3): Linear(in_features=512, out_features=102, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "metadata": {
        "id": "sTiYjiU8FHf8",
        "colab_type": "code",
        "outputId": "a115e988-b661-4976-84ec-ab5914cda3b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50705
        }
      },
      "cell_type": "code",
      "source": [
        "n_epochs = 200\n",
        "steps = 0\n",
        "every_step = 1\n",
        "train_counter = 0\n",
        "train_loss = 0.0\n",
        "valid_loss_min = np.Inf\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "    \n",
        "    steps += 1\n",
        "    \n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    \n",
        "    for data, target in trainLoader:\n",
        "      \n",
        "      train_counter += 1\n",
        "      print(\"Currect Training Batch: \", train_counter)\n",
        "\n",
        "      data,target = data.to(device),target.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      output = model(data)\n",
        "      loss = criterion(output, target)\n",
        "      loss.backward()\n",
        "      #optimizer.step()\n",
        "      scheduler.step()\n",
        "\n",
        "      train_loss += loss.item()\n",
        "        \n",
        "    ###################### \n",
        "    # validate the model #\n",
        "    ######################\n",
        "\n",
        "    if steps % every_step == 0:\n",
        "      model.eval()\n",
        "      \n",
        "      valid_loss = 0.0\n",
        "      accuracy = 0\n",
        "      \n",
        "      with torch.no_grad():\n",
        "      \n",
        "        for data, target in validLoader:\n",
        "\n",
        "            validation_counter += 1\n",
        "            print(\"Current Validating Batch: \", validation_counter)\n",
        "\n",
        "            data,target = data.to(device),target.to(device)\n",
        "\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            ps = torch.exp(output)\n",
        "            #top_p, top_class = ps.topk(1, dim=1)\n",
        "            #equals = top_class == target.view(*top_class.shape)\n",
        "            #accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "            top_class = torch.exp(output).topk(1, dim=1)[1] \n",
        "            equals = (top_class.view(target.shape[0]) == target).type(torch.FloatTensor)\n",
        "            accuracy += torch.sum(equals)\n",
        "\n",
        "      # calculating average losses\n",
        "      train_loss = train_loss/len(trainLoader)\n",
        "      valid_loss = valid_loss/len(validLoader)\n",
        "      final_accuracy = accuracy/len(validLoader)\n",
        "\n",
        "      print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tAccuracy: {:.6f} '.format(\n",
        "          epoch, train_loss, valid_loss, final_accuracy))\n",
        "\n",
        "      # save model if validation loss has decreased\n",
        "      if valid_loss <= valid_loss_min:\n",
        "          print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "          valid_loss_min,\n",
        "          valid_loss))\n",
        "          torch.save(model.state_dict(), 'drive/My Drive/AI/flower_102_model.pt')\n",
        "          valid_loss_min = valid_loss\n",
        "          \n",
        "      train_loss = 0.0\n",
        "      model.train()\n",
        "      \n",
        "      validation_counter = 0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Currect Training Batch:  1\n",
            "Currect Training Batch:  2\n",
            "Currect Training Batch:  3\n",
            "Currect Training Batch:  4\n",
            "Currect Training Batch:  5\n",
            "Currect Training Batch:  6\n",
            "Currect Training Batch:  7\n",
            "Currect Training Batch:  8\n",
            "Currect Training Batch:  9\n",
            "Currect Training Batch:  10\n",
            "Currect Training Batch:  11\n",
            "Currect Training Batch:  12\n",
            "Currect Training Batch:  13\n",
            "Currect Training Batch:  14\n",
            "Currect Training Batch:  15\n",
            "Currect Training Batch:  16\n",
            "Currect Training Batch:  17\n",
            "Currect Training Batch:  18\n",
            "Currect Training Batch:  19\n",
            "Currect Training Batch:  20\n",
            "Currect Training Batch:  21\n",
            "Currect Training Batch:  22\n",
            "Currect Training Batch:  23\n",
            "Currect Training Batch:  24\n",
            "Currect Training Batch:  25\n",
            "Currect Training Batch:  26\n",
            "Currect Training Batch:  27\n",
            "Currect Training Batch:  28\n",
            "Currect Training Batch:  29\n",
            "Currect Training Batch:  30\n",
            "Currect Training Batch:  31\n",
            "Currect Training Batch:  32\n",
            "Currect Training Batch:  33\n",
            "Currect Training Batch:  34\n",
            "Currect Training Batch:  35\n",
            "Currect Training Batch:  36\n",
            "Currect Training Batch:  37\n",
            "Currect Training Batch:  38\n",
            "Currect Training Batch:  39\n",
            "Currect Training Batch:  40\n",
            "Currect Training Batch:  41\n",
            "Currect Training Batch:  42\n",
            "Currect Training Batch:  43\n",
            "Currect Training Batch:  44\n",
            "Currect Training Batch:  45\n",
            "Currect Training Batch:  46\n",
            "Currect Training Batch:  47\n",
            "Currect Training Batch:  48\n",
            "Currect Training Batch:  49\n",
            "Currect Training Batch:  50\n",
            "Currect Training Batch:  51\n",
            "Currect Training Batch:  52\n",
            "Currect Training Batch:  53\n",
            "Currect Training Batch:  54\n",
            "Currect Training Batch:  55\n",
            "Currect Training Batch:  56\n",
            "Currect Training Batch:  57\n",
            "Currect Training Batch:  58\n",
            "Currect Training Batch:  59\n",
            "Currect Training Batch:  60\n",
            "Currect Training Batch:  61\n",
            "Currect Training Batch:  62\n",
            "Currect Training Batch:  63\n",
            "Currect Training Batch:  64\n",
            "Currect Training Batch:  65\n",
            "Currect Training Batch:  66\n",
            "Currect Training Batch:  67\n",
            "Currect Training Batch:  68\n",
            "Currect Training Batch:  69\n",
            "Currect Training Batch:  70\n",
            "Currect Training Batch:  71\n",
            "Currect Training Batch:  72\n",
            "Currect Training Batch:  73\n",
            "Currect Training Batch:  74\n",
            "Currect Training Batch:  75\n",
            "Currect Training Batch:  76\n",
            "Currect Training Batch:  77\n",
            "Currect Training Batch:  78\n",
            "Currect Training Batch:  79\n",
            "Currect Training Batch:  80\n",
            "Currect Training Batch:  81\n",
            "Currect Training Batch:  82\n",
            "Currect Training Batch:  83\n",
            "Currect Training Batch:  84\n",
            "Currect Training Batch:  85\n",
            "Currect Training Batch:  86\n",
            "Currect Training Batch:  87\n",
            "Currect Training Batch:  88\n",
            "Currect Training Batch:  89\n",
            "Currect Training Batch:  90\n",
            "Currect Training Batch:  91\n",
            "Currect Training Batch:  92\n",
            "Currect Training Batch:  93\n",
            "Currect Training Batch:  94\n",
            "Currect Training Batch:  95\n",
            "Currect Training Batch:  96\n",
            "Currect Training Batch:  97\n",
            "Currect Training Batch:  98\n",
            "Currect Training Batch:  99\n",
            "Currect Training Batch:  100\n",
            "Currect Training Batch:  101\n",
            "Currect Training Batch:  102\n",
            "Currect Training Batch:  103\n",
            "Current Validating Batch:  1\n",
            "Current Validating Batch:  2\n",
            "Current Validating Batch:  3\n",
            "Current Validating Batch:  4\n",
            "Current Validating Batch:  5\n",
            "Current Validating Batch:  6\n",
            "Current Validating Batch:  7\n",
            "Current Validating Batch:  8\n",
            "Current Validating Batch:  9\n",
            "Current Validating Batch:  10\n",
            "Current Validating Batch:  11\n",
            "Current Validating Batch:  12\n",
            "Current Validating Batch:  13\n",
            "Current Validating Batch:  14\n",
            "Current Validating Batch:  15\n",
            "Current Validating Batch:  16\n",
            "Current Validating Batch:  17\n",
            "Current Validating Batch:  18\n",
            "Current Validating Batch:  19\n",
            "Current Validating Batch:  20\n",
            "Current Validating Batch:  21\n",
            "Current Validating Batch:  22\n",
            "Current Validating Batch:  23\n",
            "Current Validating Batch:  24\n",
            "Current Validating Batch:  25\n",
            "Current Validating Batch:  26\n",
            "Current Validating Batch:  27\n",
            "Current Validating Batch:  28\n",
            "Current Validating Batch:  29\n",
            "Epoch: 1 \tTraining Loss: 0.072357 \tValidation Loss: 0.145035 \tAccuracy: 0.002165 \n",
            "Validation loss decreased (inf --> 0.145035).  Saving model ...\n",
            "Currect Training Batch:  104\n",
            "Currect Training Batch:  105\n",
            "Currect Training Batch:  106\n",
            "Currect Training Batch:  107\n",
            "Currect Training Batch:  108\n",
            "Currect Training Batch:  109\n",
            "Currect Training Batch:  110\n",
            "Currect Training Batch:  111\n",
            "Currect Training Batch:  112\n",
            "Currect Training Batch:  113\n",
            "Currect Training Batch:  114\n",
            "Currect Training Batch:  115\n",
            "Currect Training Batch:  116\n",
            "Currect Training Batch:  117\n",
            "Currect Training Batch:  118\n",
            "Currect Training Batch:  119\n",
            "Currect Training Batch:  120\n",
            "Currect Training Batch:  121\n",
            "Currect Training Batch:  122\n",
            "Currect Training Batch:  123\n",
            "Currect Training Batch:  124\n",
            "Currect Training Batch:  125\n",
            "Currect Training Batch:  126\n",
            "Currect Training Batch:  127\n",
            "Currect Training Batch:  128\n",
            "Currect Training Batch:  129\n",
            "Currect Training Batch:  130\n",
            "Currect Training Batch:  131\n",
            "Currect Training Batch:  132\n",
            "Currect Training Batch:  133\n",
            "Currect Training Batch:  134\n",
            "Currect Training Batch:  135\n",
            "Currect Training Batch:  136\n",
            "Currect Training Batch:  137\n",
            "Currect Training Batch:  138\n",
            "Currect Training Batch:  139\n",
            "Currect Training Batch:  140\n",
            "Currect Training Batch:  141\n",
            "Currect Training Batch:  142\n",
            "Currect Training Batch:  143\n",
            "Currect Training Batch:  144\n",
            "Currect Training Batch:  145\n",
            "Currect Training Batch:  146\n",
            "Currect Training Batch:  147\n",
            "Currect Training Batch:  148\n",
            "Currect Training Batch:  149\n",
            "Currect Training Batch:  150\n",
            "Currect Training Batch:  151\n",
            "Currect Training Batch:  152\n",
            "Currect Training Batch:  153\n",
            "Currect Training Batch:  154\n",
            "Currect Training Batch:  155\n",
            "Currect Training Batch:  156\n",
            "Currect Training Batch:  157\n",
            "Currect Training Batch:  158\n",
            "Currect Training Batch:  159\n",
            "Currect Training Batch:  160\n",
            "Currect Training Batch:  161\n",
            "Currect Training Batch:  162\n",
            "Currect Training Batch:  163\n",
            "Currect Training Batch:  164\n",
            "Currect Training Batch:  165\n",
            "Currect Training Batch:  166\n",
            "Currect Training Batch:  167\n",
            "Currect Training Batch:  168\n",
            "Currect Training Batch:  169\n",
            "Currect Training Batch:  170\n",
            "Currect Training Batch:  171\n",
            "Currect Training Batch:  172\n",
            "Currect Training Batch:  173\n",
            "Currect Training Batch:  174\n",
            "Currect Training Batch:  175\n",
            "Currect Training Batch:  176\n",
            "Currect Training Batch:  177\n",
            "Currect Training Batch:  178\n",
            "Currect Training Batch:  179\n",
            "Currect Training Batch:  180\n",
            "Currect Training Batch:  181\n",
            "Currect Training Batch:  182\n",
            "Currect Training Batch:  183\n",
            "Currect Training Batch:  184\n",
            "Currect Training Batch:  185\n",
            "Currect Training Batch:  186\n",
            "Currect Training Batch:  187\n",
            "Currect Training Batch:  188\n",
            "Currect Training Batch:  189\n",
            "Currect Training Batch:  190\n",
            "Currect Training Batch:  191\n",
            "Currect Training Batch:  192\n",
            "Currect Training Batch:  193\n",
            "Currect Training Batch:  194\n",
            "Currect Training Batch:  195\n",
            "Currect Training Batch:  196\n",
            "Currect Training Batch:  197\n",
            "Currect Training Batch:  198\n",
            "Currect Training Batch:  199\n",
            "Currect Training Batch:  200\n",
            "Currect Training Batch:  201\n",
            "Currect Training Batch:  202\n",
            "Currect Training Batch:  203\n",
            "Currect Training Batch:  204\n",
            "Currect Training Batch:  205\n",
            "Currect Training Batch:  206\n",
            "Current Validating Batch:  1\n",
            "Current Validating Batch:  2\n",
            "Current Validating Batch:  3\n",
            "Current Validating Batch:  4\n",
            "Current Validating Batch:  5\n",
            "Current Validating Batch:  6\n",
            "Current Validating Batch:  7\n",
            "Current Validating Batch:  8\n",
            "Current Validating Batch:  9\n",
            "Current Validating Batch:  10\n",
            "Current Validating Batch:  11\n",
            "Current Validating Batch:  12\n",
            "Current Validating Batch:  13\n",
            "Current Validating Batch:  14\n",
            "Current Validating Batch:  15\n",
            "Current Validating Batch:  16\n",
            "Current Validating Batch:  17\n",
            "Current Validating Batch:  18\n",
            "Current Validating Batch:  19\n",
            "Current Validating Batch:  20\n",
            "Current Validating Batch:  21\n",
            "Current Validating Batch:  22\n",
            "Current Validating Batch:  23\n",
            "Current Validating Batch:  24\n",
            "Current Validating Batch:  25\n",
            "Current Validating Batch:  26\n",
            "Current Validating Batch:  27\n",
            "Current Validating Batch:  28\n",
            "Current Validating Batch:  29\n",
            "Epoch: 2 \tTraining Loss: 0.072338 \tValidation Loss: 0.145020 \tAccuracy: 0.001082 \n",
            "Validation loss decreased (0.145035 --> 0.145020).  Saving model ...\n",
            "Currect Training Batch:  207\n",
            "Currect Training Batch:  208\n",
            "Currect Training Batch:  209\n",
            "Currect Training Batch:  210\n",
            "Currect Training Batch:  211\n",
            "Currect Training Batch:  212\n",
            "Currect Training Batch:  213\n",
            "Currect Training Batch:  214\n",
            "Currect Training Batch:  215\n",
            "Currect Training Batch:  216\n",
            "Currect Training Batch:  217\n",
            "Currect Training Batch:  218\n",
            "Currect Training Batch:  219\n",
            "Currect Training Batch:  220\n",
            "Currect Training Batch:  221\n",
            "Currect Training Batch:  222\n",
            "Currect Training Batch:  223\n",
            "Currect Training Batch:  224\n",
            "Currect Training Batch:  225\n",
            "Currect Training Batch:  226\n",
            "Currect Training Batch:  227\n",
            "Currect Training Batch:  228\n",
            "Currect Training Batch:  229\n",
            "Currect Training Batch:  230\n",
            "Currect Training Batch:  231\n",
            "Currect Training Batch:  232\n",
            "Currect Training Batch:  233\n",
            "Currect Training Batch:  234\n",
            "Currect Training Batch:  235\n",
            "Currect Training Batch:  236\n",
            "Currect Training Batch:  237\n",
            "Currect Training Batch:  238\n",
            "Currect Training Batch:  239\n",
            "Currect Training Batch:  240\n",
            "Currect Training Batch:  241\n",
            "Currect Training Batch:  242\n",
            "Currect Training Batch:  243\n",
            "Currect Training Batch:  244\n",
            "Currect Training Batch:  245\n",
            "Currect Training Batch:  246\n",
            "Currect Training Batch:  247\n",
            "Currect Training Batch:  248\n",
            "Currect Training Batch:  249\n",
            "Currect Training Batch:  250\n",
            "Currect Training Batch:  251\n",
            "Currect Training Batch:  252\n",
            "Currect Training Batch:  253\n",
            "Currect Training Batch:  254\n",
            "Currect Training Batch:  255\n",
            "Currect Training Batch:  256\n",
            "Currect Training Batch:  257\n",
            "Currect Training Batch:  258\n",
            "Currect Training Batch:  259\n",
            "Currect Training Batch:  260\n",
            "Currect Training Batch:  261\n",
            "Currect Training Batch:  262\n",
            "Currect Training Batch:  263\n",
            "Currect Training Batch:  264\n",
            "Currect Training Batch:  265\n",
            "Currect Training Batch:  266\n",
            "Currect Training Batch:  267\n",
            "Currect Training Batch:  268\n",
            "Currect Training Batch:  269\n",
            "Currect Training Batch:  270\n",
            "Currect Training Batch:  271\n",
            "Currect Training Batch:  272\n",
            "Currect Training Batch:  273\n",
            "Currect Training Batch:  274\n",
            "Currect Training Batch:  275\n",
            "Currect Training Batch:  276\n",
            "Currect Training Batch:  277\n",
            "Currect Training Batch:  278\n",
            "Currect Training Batch:  279\n",
            "Currect Training Batch:  280\n",
            "Currect Training Batch:  281\n",
            "Currect Training Batch:  282\n",
            "Currect Training Batch:  283\n",
            "Currect Training Batch:  284\n",
            "Currect Training Batch:  285\n",
            "Currect Training Batch:  286\n",
            "Currect Training Batch:  287\n",
            "Currect Training Batch:  288\n",
            "Currect Training Batch:  289\n",
            "Currect Training Batch:  290\n",
            "Currect Training Batch:  291\n",
            "Currect Training Batch:  292\n",
            "Currect Training Batch:  293\n",
            "Currect Training Batch:  294\n",
            "Currect Training Batch:  295\n",
            "Currect Training Batch:  296\n",
            "Currect Training Batch:  297\n",
            "Currect Training Batch:  298\n",
            "Currect Training Batch:  299\n",
            "Currect Training Batch:  300\n",
            "Currect Training Batch:  301\n",
            "Currect Training Batch:  302\n",
            "Currect Training Batch:  303\n",
            "Currect Training Batch:  304\n",
            "Currect Training Batch:  305\n",
            "Currect Training Batch:  306\n",
            "Currect Training Batch:  307\n",
            "Currect Training Batch:  308\n",
            "Currect Training Batch:  309\n",
            "Current Validating Batch:  1\n",
            "Current Validating Batch:  2\n",
            "Current Validating Batch:  3\n",
            "Current Validating Batch:  4\n",
            "Current Validating Batch:  5\n",
            "Current Validating Batch:  6\n",
            "Current Validating Batch:  7\n",
            "Current Validating Batch:  8\n",
            "Current Validating Batch:  9\n",
            "Current Validating Batch:  10\n",
            "Current Validating Batch:  11\n",
            "Current Validating Batch:  12\n",
            "Current Validating Batch:  13\n",
            "Current Validating Batch:  14\n",
            "Current Validating Batch:  15\n",
            "Current Validating Batch:  16\n",
            "Current Validating Batch:  17\n",
            "Current Validating Batch:  18\n",
            "Current Validating Batch:  19\n",
            "Current Validating Batch:  20\n",
            "Current Validating Batch:  21\n",
            "Current Validating Batch:  22\n",
            "Current Validating Batch:  23\n",
            "Current Validating Batch:  24\n",
            "Current Validating Batch:  25\n",
            "Current Validating Batch:  26\n",
            "Current Validating Batch:  27\n",
            "Current Validating Batch:  28\n",
            "Current Validating Batch:  29\n",
            "Epoch: 3 \tTraining Loss: 0.072357 \tValidation Loss: 0.145032 \tAccuracy: 0.001082 \n",
            "Currect Training Batch:  310\n",
            "Currect Training Batch:  311\n",
            "Currect Training Batch:  312\n",
            "Currect Training Batch:  313\n",
            "Currect Training Batch:  314\n",
            "Currect Training Batch:  315\n",
            "Currect Training Batch:  316\n",
            "Currect Training Batch:  317\n",
            "Currect Training Batch:  318\n",
            "Currect Training Batch:  319\n",
            "Currect Training Batch:  320\n",
            "Currect Training Batch:  321\n",
            "Currect Training Batch:  322\n",
            "Currect Training Batch:  323\n",
            "Currect Training Batch:  324\n",
            "Currect Training Batch:  325\n",
            "Currect Training Batch:  326\n",
            "Currect Training Batch:  327\n",
            "Currect Training Batch:  328\n",
            "Currect Training Batch:  329\n",
            "Currect Training Batch:  330\n",
            "Currect Training Batch:  331\n",
            "Currect Training Batch:  332\n",
            "Currect Training Batch:  333\n",
            "Currect Training Batch:  334\n",
            "Currect Training Batch:  335\n",
            "Currect Training Batch:  336\n",
            "Currect Training Batch:  337\n",
            "Currect Training Batch:  338\n",
            "Currect Training Batch:  339\n",
            "Currect Training Batch:  340\n",
            "Currect Training Batch:  341\n",
            "Currect Training Batch:  342\n",
            "Currect Training Batch:  343\n",
            "Currect Training Batch:  344\n",
            "Currect Training Batch:  345\n",
            "Currect Training Batch:  346\n",
            "Currect Training Batch:  347\n",
            "Currect Training Batch:  348\n",
            "Currect Training Batch:  349\n",
            "Currect Training Batch:  350\n",
            "Currect Training Batch:  351\n",
            "Currect Training Batch:  352\n",
            "Currect Training Batch:  353\n",
            "Currect Training Batch:  354\n",
            "Currect Training Batch:  355\n",
            "Currect Training Batch:  356\n",
            "Currect Training Batch:  357\n",
            "Currect Training Batch:  358\n",
            "Currect Training Batch:  359\n",
            "Currect Training Batch:  360\n",
            "Currect Training Batch:  361\n",
            "Currect Training Batch:  362\n",
            "Currect Training Batch:  363\n",
            "Currect Training Batch:  364\n",
            "Currect Training Batch:  365\n",
            "Currect Training Batch:  366\n",
            "Currect Training Batch:  367\n",
            "Currect Training Batch:  368\n",
            "Currect Training Batch:  369\n",
            "Currect Training Batch:  370\n",
            "Currect Training Batch:  371\n",
            "Currect Training Batch:  372\n",
            "Currect Training Batch:  373\n",
            "Currect Training Batch:  374\n",
            "Currect Training Batch:  375\n",
            "Currect Training Batch:  376\n",
            "Currect Training Batch:  377\n",
            "Currect Training Batch:  378\n",
            "Currect Training Batch:  379\n",
            "Currect Training Batch:  380\n",
            "Currect Training Batch:  381\n",
            "Currect Training Batch:  382\n",
            "Currect Training Batch:  383\n",
            "Currect Training Batch:  384\n",
            "Currect Training Batch:  385\n",
            "Currect Training Batch:  386\n",
            "Currect Training Batch:  387\n",
            "Currect Training Batch:  388\n",
            "Currect Training Batch:  389\n",
            "Currect Training Batch:  390\n",
            "Currect Training Batch:  391\n",
            "Currect Training Batch:  392\n",
            "Currect Training Batch:  393\n",
            "Currect Training Batch:  394\n",
            "Currect Training Batch:  395\n",
            "Currect Training Batch:  396\n",
            "Currect Training Batch:  397\n",
            "Currect Training Batch:  398\n",
            "Currect Training Batch:  399\n",
            "Currect Training Batch:  400\n",
            "Currect Training Batch:  401\n",
            "Currect Training Batch:  402\n",
            "Currect Training Batch:  403\n",
            "Currect Training Batch:  404\n",
            "Currect Training Batch:  405\n",
            "Currect Training Batch:  406\n",
            "Currect Training Batch:  407\n",
            "Currect Training Batch:  408\n",
            "Currect Training Batch:  409\n",
            "Currect Training Batch:  410\n",
            "Currect Training Batch:  411\n",
            "Currect Training Batch:  412\n",
            "Current Validating Batch:  1\n",
            "Current Validating Batch:  2\n",
            "Current Validating Batch:  3\n",
            "Current Validating Batch:  4\n",
            "Current Validating Batch:  5\n",
            "Current Validating Batch:  6\n",
            "Current Validating Batch:  7\n",
            "Current Validating Batch:  8\n",
            "Current Validating Batch:  9\n",
            "Current Validating Batch:  10\n",
            "Current Validating Batch:  11\n",
            "Current Validating Batch:  12\n",
            "Current Validating Batch:  13\n",
            "Current Validating Batch:  14\n",
            "Current Validating Batch:  15\n",
            "Current Validating Batch:  16\n",
            "Current Validating Batch:  17\n",
            "Current Validating Batch:  18\n",
            "Current Validating Batch:  19\n",
            "Current Validating Batch:  20\n",
            "Current Validating Batch:  21\n",
            "Current Validating Batch:  22\n",
            "Current Validating Batch:  23\n",
            "Current Validating Batch:  24\n",
            "Current Validating Batch:  25\n",
            "Current Validating Batch:  26\n",
            "Current Validating Batch:  27\n",
            "Current Validating Batch:  28\n",
            "Current Validating Batch:  29\n",
            "Epoch: 4 \tTraining Loss: 0.072310 \tValidation Loss: 0.145024 \tAccuracy: 0.001082 \n",
            "Currect Training Batch:  413\n",
            "Currect Training Batch:  414\n",
            "Currect Training Batch:  415\n",
            "Currect Training Batch:  416\n",
            "Currect Training Batch:  417\n",
            "Currect Training Batch:  418\n",
            "Currect Training Batch:  419\n",
            "Currect Training Batch:  420\n",
            "Currect Training Batch:  421\n",
            "Currect Training Batch:  422\n",
            "Currect Training Batch:  423\n",
            "Currect Training Batch:  424\n",
            "Currect Training Batch:  425\n",
            "Currect Training Batch:  426\n",
            "Currect Training Batch:  427\n",
            "Currect Training Batch:  428\n",
            "Currect Training Batch:  429\n",
            "Currect Training Batch:  430\n",
            "Currect Training Batch:  431\n",
            "Currect Training Batch:  432\n",
            "Currect Training Batch:  433\n",
            "Currect Training Batch:  434\n",
            "Currect Training Batch:  435\n",
            "Currect Training Batch:  436\n",
            "Currect Training Batch:  437\n",
            "Currect Training Batch:  438\n",
            "Currect Training Batch:  439\n",
            "Currect Training Batch:  440\n",
            "Currect Training Batch:  441\n",
            "Currect Training Batch:  442\n",
            "Currect Training Batch:  443\n",
            "Currect Training Batch:  444\n",
            "Currect Training Batch:  445\n",
            "Currect Training Batch:  446\n",
            "Currect Training Batch:  447\n",
            "Currect Training Batch:  448\n",
            "Currect Training Batch:  449\n",
            "Currect Training Batch:  450\n",
            "Currect Training Batch:  451\n",
            "Currect Training Batch:  452\n",
            "Currect Training Batch:  453\n",
            "Currect Training Batch:  454\n",
            "Currect Training Batch:  455\n",
            "Currect Training Batch:  456\n",
            "Currect Training Batch:  457\n",
            "Currect Training Batch:  458\n",
            "Currect Training Batch:  459\n",
            "Currect Training Batch:  460\n",
            "Currect Training Batch:  461\n",
            "Currect Training Batch:  462\n",
            "Currect Training Batch:  463\n",
            "Currect Training Batch:  464\n",
            "Currect Training Batch:  465\n",
            "Currect Training Batch:  466\n",
            "Currect Training Batch:  467\n",
            "Currect Training Batch:  468\n",
            "Currect Training Batch:  469\n",
            "Currect Training Batch:  470\n",
            "Currect Training Batch:  471\n",
            "Currect Training Batch:  472\n",
            "Currect Training Batch:  473\n",
            "Currect Training Batch:  474\n",
            "Currect Training Batch:  475\n",
            "Currect Training Batch:  476\n",
            "Currect Training Batch:  477\n",
            "Currect Training Batch:  478\n",
            "Currect Training Batch:  479\n",
            "Currect Training Batch:  480\n",
            "Currect Training Batch:  481\n",
            "Currect Training Batch:  482\n",
            "Currect Training Batch:  483\n",
            "Currect Training Batch:  484\n",
            "Currect Training Batch:  485\n",
            "Currect Training Batch:  486\n",
            "Currect Training Batch:  487\n",
            "Currect Training Batch:  488\n",
            "Currect Training Batch:  489\n",
            "Currect Training Batch:  490\n",
            "Currect Training Batch:  491\n",
            "Currect Training Batch:  492\n",
            "Currect Training Batch:  493\n",
            "Currect Training Batch:  494\n",
            "Currect Training Batch:  495\n",
            "Currect Training Batch:  496\n",
            "Currect Training Batch:  497\n",
            "Currect Training Batch:  498\n",
            "Currect Training Batch:  499\n",
            "Currect Training Batch:  500\n",
            "Currect Training Batch:  501\n",
            "Currect Training Batch:  502\n",
            "Currect Training Batch:  503\n",
            "Currect Training Batch:  504\n",
            "Currect Training Batch:  505\n",
            "Currect Training Batch:  506\n",
            "Currect Training Batch:  507\n",
            "Currect Training Batch:  508\n",
            "Currect Training Batch:  509\n",
            "Currect Training Batch:  510\n",
            "Currect Training Batch:  511\n",
            "Currect Training Batch:  512\n",
            "Currect Training Batch:  513\n",
            "Currect Training Batch:  514\n",
            "Currect Training Batch:  515\n",
            "Current Validating Batch:  1\n",
            "Current Validating Batch:  2\n",
            "Current Validating Batch:  3\n",
            "Current Validating Batch:  4\n",
            "Current Validating Batch:  5\n",
            "Current Validating Batch:  6\n",
            "Current Validating Batch:  7\n",
            "Current Validating Batch:  8\n",
            "Current Validating Batch:  9\n",
            "Current Validating Batch:  10\n",
            "Current Validating Batch:  11\n",
            "Current Validating Batch:  12\n",
            "Current Validating Batch:  13\n",
            "Current Validating Batch:  14\n",
            "Current Validating Batch:  15\n",
            "Current Validating Batch:  16\n",
            "Current Validating Batch:  17\n",
            "Current Validating Batch:  18\n",
            "Current Validating Batch:  19\n",
            "Current Validating Batch:  20\n",
            "Current Validating Batch:  21\n",
            "Current Validating Batch:  22\n",
            "Current Validating Batch:  23\n",
            "Current Validating Batch:  24\n",
            "Current Validating Batch:  25\n",
            "Current Validating Batch:  26\n",
            "Current Validating Batch:  27\n",
            "Current Validating Batch:  28\n",
            "Current Validating Batch:  29\n",
            "Epoch: 5 \tTraining Loss: 0.072355 \tValidation Loss: 0.145018 \tAccuracy: 0.001082 \n",
            "Validation loss decreased (0.145020 --> 0.145018).  Saving model ...\n",
            "Currect Training Batch:  516\n",
            "Currect Training Batch:  517\n",
            "Currect Training Batch:  518\n",
            "Currect Training Batch:  519\n",
            "Currect Training Batch:  520\n",
            "Currect Training Batch:  521\n",
            "Currect Training Batch:  522\n",
            "Currect Training Batch:  523\n",
            "Currect Training Batch:  524\n",
            "Currect Training Batch:  525\n",
            "Currect Training Batch:  526\n",
            "Currect Training Batch:  527\n",
            "Currect Training Batch:  528\n",
            "Currect Training Batch:  529\n",
            "Currect Training Batch:  530\n",
            "Currect Training Batch:  531\n",
            "Currect Training Batch:  532\n",
            "Currect Training Batch:  533\n",
            "Currect Training Batch:  534\n",
            "Currect Training Batch:  535\n",
            "Currect Training Batch:  536\n",
            "Currect Training Batch:  537\n",
            "Currect Training Batch:  538\n",
            "Currect Training Batch:  539\n",
            "Currect Training Batch:  540\n",
            "Currect Training Batch:  541\n",
            "Currect Training Batch:  542\n",
            "Currect Training Batch:  543\n",
            "Currect Training Batch:  544\n",
            "Currect Training Batch:  545\n",
            "Currect Training Batch:  546\n",
            "Currect Training Batch:  547\n",
            "Currect Training Batch:  548\n",
            "Currect Training Batch:  549\n",
            "Currect Training Batch:  550\n",
            "Currect Training Batch:  551\n",
            "Currect Training Batch:  552\n",
            "Currect Training Batch:  553\n",
            "Currect Training Batch:  554\n",
            "Currect Training Batch:  555\n",
            "Currect Training Batch:  556\n",
            "Currect Training Batch:  557\n",
            "Currect Training Batch:  558\n",
            "Currect Training Batch:  559\n",
            "Currect Training Batch:  560\n",
            "Currect Training Batch:  561\n",
            "Currect Training Batch:  562\n",
            "Currect Training Batch:  563\n",
            "Currect Training Batch:  564\n",
            "Currect Training Batch:  565\n",
            "Currect Training Batch:  566\n",
            "Currect Training Batch:  567\n",
            "Currect Training Batch:  568\n",
            "Currect Training Batch:  569\n",
            "Currect Training Batch:  570\n",
            "Currect Training Batch:  571\n",
            "Currect Training Batch:  572\n",
            "Currect Training Batch:  573\n",
            "Currect Training Batch:  574\n",
            "Currect Training Batch:  575\n",
            "Currect Training Batch:  576\n",
            "Currect Training Batch:  577\n",
            "Currect Training Batch:  578\n",
            "Currect Training Batch:  579\n",
            "Currect Training Batch:  580\n",
            "Currect Training Batch:  581\n",
            "Currect Training Batch:  582\n",
            "Currect Training Batch:  583\n",
            "Currect Training Batch:  584\n",
            "Currect Training Batch:  585\n",
            "Currect Training Batch:  586\n",
            "Currect Training Batch:  587\n",
            "Currect Training Batch:  588\n",
            "Currect Training Batch:  589\n",
            "Currect Training Batch:  590\n",
            "Currect Training Batch:  591\n",
            "Currect Training Batch:  592\n",
            "Currect Training Batch:  593\n",
            "Currect Training Batch:  594\n",
            "Currect Training Batch:  595\n",
            "Currect Training Batch:  596\n",
            "Currect Training Batch:  597\n",
            "Currect Training Batch:  598\n",
            "Currect Training Batch:  599\n",
            "Currect Training Batch:  600\n",
            "Currect Training Batch:  601\n",
            "Currect Training Batch:  602\n",
            "Currect Training Batch:  603\n",
            "Currect Training Batch:  604\n",
            "Currect Training Batch:  605\n",
            "Currect Training Batch:  606\n",
            "Currect Training Batch:  607\n",
            "Currect Training Batch:  608\n",
            "Currect Training Batch:  609\n",
            "Currect Training Batch:  610\n",
            "Currect Training Batch:  611\n",
            "Currect Training Batch:  612\n",
            "Currect Training Batch:  613\n",
            "Currect Training Batch:  614\n",
            "Currect Training Batch:  615\n",
            "Currect Training Batch:  616\n",
            "Currect Training Batch:  617\n",
            "Currect Training Batch:  618\n",
            "Current Validating Batch:  1\n",
            "Current Validating Batch:  2\n",
            "Current Validating Batch:  3\n",
            "Current Validating Batch:  4\n",
            "Current Validating Batch:  5\n",
            "Current Validating Batch:  6\n",
            "Current Validating Batch:  7\n",
            "Current Validating Batch:  8\n",
            "Current Validating Batch:  9\n",
            "Current Validating Batch:  10\n",
            "Current Validating Batch:  11\n",
            "Current Validating Batch:  12\n",
            "Current Validating Batch:  13\n",
            "Current Validating Batch:  14\n",
            "Current Validating Batch:  15\n",
            "Current Validating Batch:  16\n",
            "Current Validating Batch:  17\n",
            "Current Validating Batch:  18\n",
            "Current Validating Batch:  19\n",
            "Current Validating Batch:  20\n",
            "Current Validating Batch:  21\n",
            "Current Validating Batch:  22\n",
            "Current Validating Batch:  23\n",
            "Current Validating Batch:  24\n",
            "Current Validating Batch:  25\n",
            "Current Validating Batch:  26\n",
            "Current Validating Batch:  27\n",
            "Current Validating Batch:  28\n",
            "Current Validating Batch:  29\n",
            "Epoch: 6 \tTraining Loss: 0.072331 \tValidation Loss: 0.145016 \tAccuracy: 0.001082 \n",
            "Validation loss decreased (0.145018 --> 0.145016).  Saving model ...\n",
            "Currect Training Batch:  619\n",
            "Currect Training Batch:  620\n",
            "Currect Training Batch:  621\n",
            "Currect Training Batch:  622\n",
            "Currect Training Batch:  623\n",
            "Currect Training Batch:  624\n",
            "Currect Training Batch:  625\n",
            "Currect Training Batch:  626\n",
            "Currect Training Batch:  627\n",
            "Currect Training Batch:  628\n",
            "Currect Training Batch:  629\n",
            "Currect Training Batch:  630\n",
            "Currect Training Batch:  631\n",
            "Currect Training Batch:  632\n",
            "Currect Training Batch:  633\n",
            "Currect Training Batch:  634\n",
            "Currect Training Batch:  635\n",
            "Currect Training Batch:  636\n",
            "Currect Training Batch:  637\n",
            "Currect Training Batch:  638\n",
            "Currect Training Batch:  639\n",
            "Currect Training Batch:  640\n",
            "Currect Training Batch:  641\n",
            "Currect Training Batch:  642\n",
            "Currect Training Batch:  643\n",
            "Currect Training Batch:  644\n",
            "Currect Training Batch:  645\n",
            "Currect Training Batch:  646\n",
            "Currect Training Batch:  647\n",
            "Currect Training Batch:  648\n",
            "Currect Training Batch:  649\n",
            "Currect Training Batch:  650\n",
            "Currect Training Batch:  651\n",
            "Currect Training Batch:  652\n",
            "Currect Training Batch:  653\n",
            "Currect Training Batch:  654\n",
            "Currect Training Batch:  655\n",
            "Currect Training Batch:  656\n",
            "Currect Training Batch:  657\n",
            "Currect Training Batch:  658\n",
            "Currect Training Batch:  659\n",
            "Currect Training Batch:  660\n",
            "Currect Training Batch:  661\n",
            "Currect Training Batch:  662\n",
            "Currect Training Batch:  663\n",
            "Currect Training Batch:  664\n",
            "Currect Training Batch:  665\n",
            "Currect Training Batch:  666\n",
            "Currect Training Batch:  667\n",
            "Currect Training Batch:  668\n",
            "Currect Training Batch:  669\n",
            "Currect Training Batch:  670\n",
            "Currect Training Batch:  671\n",
            "Currect Training Batch:  672\n",
            "Currect Training Batch:  673\n",
            "Currect Training Batch:  674\n",
            "Currect Training Batch:  675\n",
            "Currect Training Batch:  676\n",
            "Currect Training Batch:  677\n",
            "Currect Training Batch:  678\n",
            "Currect Training Batch:  679\n",
            "Currect Training Batch:  680\n",
            "Currect Training Batch:  681\n",
            "Currect Training Batch:  682\n",
            "Currect Training Batch:  683\n",
            "Currect Training Batch:  684\n",
            "Currect Training Batch:  685\n",
            "Currect Training Batch:  686\n",
            "Currect Training Batch:  687\n",
            "Currect Training Batch:  688\n",
            "Currect Training Batch:  689\n",
            "Currect Training Batch:  690\n",
            "Currect Training Batch:  691\n",
            "Currect Training Batch:  692\n",
            "Currect Training Batch:  693\n",
            "Currect Training Batch:  694\n",
            "Currect Training Batch:  695\n",
            "Currect Training Batch:  696\n",
            "Currect Training Batch:  697\n",
            "Currect Training Batch:  698\n",
            "Currect Training Batch:  699\n",
            "Currect Training Batch:  700\n",
            "Currect Training Batch:  701\n",
            "Currect Training Batch:  702\n",
            "Currect Training Batch:  703\n",
            "Currect Training Batch:  704\n",
            "Currect Training Batch:  705\n",
            "Currect Training Batch:  706\n",
            "Currect Training Batch:  707\n",
            "Currect Training Batch:  708\n",
            "Currect Training Batch:  709\n",
            "Currect Training Batch:  710\n",
            "Currect Training Batch:  711\n",
            "Currect Training Batch:  712\n",
            "Currect Training Batch:  713\n",
            "Currect Training Batch:  714\n",
            "Currect Training Batch:  715\n",
            "Currect Training Batch:  716\n",
            "Currect Training Batch:  717\n",
            "Currect Training Batch:  718\n",
            "Currect Training Batch:  719\n",
            "Currect Training Batch:  720\n",
            "Currect Training Batch:  721\n",
            "Current Validating Batch:  1\n",
            "Current Validating Batch:  2\n",
            "Current Validating Batch:  3\n",
            "Current Validating Batch:  4\n",
            "Current Validating Batch:  5\n",
            "Current Validating Batch:  6\n",
            "Current Validating Batch:  7\n",
            "Current Validating Batch:  8\n",
            "Current Validating Batch:  9\n",
            "Current Validating Batch:  10\n",
            "Current Validating Batch:  11\n",
            "Current Validating Batch:  12\n",
            "Current Validating Batch:  13\n",
            "Current Validating Batch:  14\n",
            "Current Validating Batch:  15\n",
            "Current Validating Batch:  16\n",
            "Current Validating Batch:  17\n",
            "Current Validating Batch:  18\n",
            "Current Validating Batch:  19\n",
            "Current Validating Batch:  20\n",
            "Current Validating Batch:  21\n",
            "Current Validating Batch:  22\n",
            "Current Validating Batch:  23\n",
            "Current Validating Batch:  24\n",
            "Current Validating Batch:  25\n",
            "Current Validating Batch:  26\n",
            "Current Validating Batch:  27\n",
            "Current Validating Batch:  28\n",
            "Current Validating Batch:  29\n",
            "Epoch: 7 \tTraining Loss: 0.072320 \tValidation Loss: 0.145026 \tAccuracy: 0.001082 \n",
            "Currect Training Batch:  722\n",
            "Currect Training Batch:  723\n",
            "Currect Training Batch:  724\n",
            "Currect Training Batch:  725\n",
            "Currect Training Batch:  726\n",
            "Currect Training Batch:  727\n",
            "Currect Training Batch:  728\n",
            "Currect Training Batch:  729\n",
            "Currect Training Batch:  730\n",
            "Currect Training Batch:  731\n",
            "Currect Training Batch:  732\n",
            "Currect Training Batch:  733\n",
            "Currect Training Batch:  734\n",
            "Currect Training Batch:  735\n",
            "Currect Training Batch:  736\n",
            "Currect Training Batch:  737\n",
            "Currect Training Batch:  738\n",
            "Currect Training Batch:  739\n",
            "Currect Training Batch:  740\n",
            "Currect Training Batch:  741\n",
            "Currect Training Batch:  742\n",
            "Currect Training Batch:  743\n",
            "Currect Training Batch:  744\n",
            "Currect Training Batch:  745\n",
            "Currect Training Batch:  746\n",
            "Currect Training Batch:  747\n",
            "Currect Training Batch:  748\n",
            "Currect Training Batch:  749\n",
            "Currect Training Batch:  750\n",
            "Currect Training Batch:  751\n",
            "Currect Training Batch:  752\n",
            "Currect Training Batch:  753\n",
            "Currect Training Batch:  754\n",
            "Currect Training Batch:  755\n",
            "Currect Training Batch:  756\n",
            "Currect Training Batch:  757\n",
            "Currect Training Batch:  758\n",
            "Currect Training Batch:  759\n",
            "Currect Training Batch:  760\n",
            "Currect Training Batch:  761\n",
            "Currect Training Batch:  762\n",
            "Currect Training Batch:  763\n",
            "Currect Training Batch:  764\n",
            "Currect Training Batch:  765\n",
            "Currect Training Batch:  766\n",
            "Currect Training Batch:  767\n",
            "Currect Training Batch:  768\n",
            "Currect Training Batch:  769\n",
            "Currect Training Batch:  770\n",
            "Currect Training Batch:  771\n",
            "Currect Training Batch:  772\n",
            "Currect Training Batch:  773\n",
            "Currect Training Batch:  774\n",
            "Currect Training Batch:  775\n",
            "Currect Training Batch:  776\n",
            "Currect Training Batch:  777\n",
            "Currect Training Batch:  778\n",
            "Currect Training Batch:  779\n",
            "Currect Training Batch:  780\n",
            "Currect Training Batch:  781\n",
            "Currect Training Batch:  782\n",
            "Currect Training Batch:  783\n",
            "Currect Training Batch:  784\n",
            "Currect Training Batch:  785\n",
            "Currect Training Batch:  786\n",
            "Currect Training Batch:  787\n",
            "Currect Training Batch:  788\n",
            "Currect Training Batch:  789\n",
            "Currect Training Batch:  790\n",
            "Currect Training Batch:  791\n",
            "Currect Training Batch:  792\n",
            "Currect Training Batch:  793\n",
            "Currect Training Batch:  794\n",
            "Currect Training Batch:  795\n",
            "Currect Training Batch:  796\n",
            "Currect Training Batch:  797\n",
            "Currect Training Batch:  798\n",
            "Currect Training Batch:  799\n",
            "Currect Training Batch:  800\n",
            "Currect Training Batch:  801\n",
            "Currect Training Batch:  802\n",
            "Currect Training Batch:  803\n",
            "Currect Training Batch:  804\n",
            "Currect Training Batch:  805\n",
            "Currect Training Batch:  806\n",
            "Currect Training Batch:  807\n",
            "Currect Training Batch:  808\n",
            "Currect Training Batch:  809\n",
            "Currect Training Batch:  810\n",
            "Currect Training Batch:  811\n",
            "Currect Training Batch:  812\n",
            "Currect Training Batch:  813\n",
            "Currect Training Batch:  814\n",
            "Currect Training Batch:  815\n",
            "Currect Training Batch:  816\n",
            "Currect Training Batch:  817\n",
            "Currect Training Batch:  818\n",
            "Currect Training Batch:  819\n",
            "Currect Training Batch:  820\n",
            "Currect Training Batch:  821\n",
            "Currect Training Batch:  822\n",
            "Currect Training Batch:  823\n",
            "Currect Training Batch:  824\n",
            "Current Validating Batch:  1\n",
            "Current Validating Batch:  2\n",
            "Current Validating Batch:  3\n",
            "Current Validating Batch:  4\n",
            "Current Validating Batch:  5\n",
            "Current Validating Batch:  6\n",
            "Current Validating Batch:  7\n",
            "Current Validating Batch:  8\n",
            "Current Validating Batch:  9\n",
            "Current Validating Batch:  10\n",
            "Current Validating Batch:  11\n",
            "Current Validating Batch:  12\n",
            "Current Validating Batch:  13\n",
            "Current Validating Batch:  14\n",
            "Current Validating Batch:  15\n",
            "Current Validating Batch:  16\n",
            "Current Validating Batch:  17\n",
            "Current Validating Batch:  18\n",
            "Current Validating Batch:  19\n",
            "Current Validating Batch:  20\n",
            "Current Validating Batch:  21\n",
            "Current Validating Batch:  22\n",
            "Current Validating Batch:  23\n",
            "Current Validating Batch:  24\n",
            "Current Validating Batch:  25\n",
            "Current Validating Batch:  26\n",
            "Current Validating Batch:  27\n",
            "Current Validating Batch:  28\n",
            "Current Validating Batch:  29\n",
            "Epoch: 8 \tTraining Loss: 0.072336 \tValidation Loss: 0.145026 \tAccuracy: 0.002165 \n",
            "Currect Training Batch:  825\n",
            "Currect Training Batch:  826\n",
            "Currect Training Batch:  827\n",
            "Currect Training Batch:  828\n",
            "Currect Training Batch:  829\n",
            "Currect Training Batch:  830\n",
            "Currect Training Batch:  831\n",
            "Currect Training Batch:  832\n",
            "Currect Training Batch:  833\n",
            "Currect Training Batch:  834\n",
            "Currect Training Batch:  835\n",
            "Currect Training Batch:  836\n",
            "Currect Training Batch:  837\n",
            "Currect Training Batch:  838\n",
            "Currect Training Batch:  839\n",
            "Currect Training Batch:  840\n",
            "Currect Training Batch:  841\n",
            "Currect Training Batch:  842\n",
            "Currect Training Batch:  843\n",
            "Currect Training Batch:  844\n",
            "Currect Training Batch:  845\n",
            "Currect Training Batch:  846\n",
            "Currect Training Batch:  847\n",
            "Currect Training Batch:  848\n",
            "Currect Training Batch:  849\n",
            "Currect Training Batch:  850\n",
            "Currect Training Batch:  851\n",
            "Currect Training Batch:  852\n",
            "Currect Training Batch:  853\n",
            "Currect Training Batch:  854\n",
            "Currect Training Batch:  855\n",
            "Currect Training Batch:  856\n",
            "Currect Training Batch:  857\n",
            "Currect Training Batch:  858\n",
            "Currect Training Batch:  859\n",
            "Currect Training Batch:  860\n",
            "Currect Training Batch:  861\n",
            "Currect Training Batch:  862\n",
            "Currect Training Batch:  863\n",
            "Currect Training Batch:  864\n",
            "Currect Training Batch:  865\n",
            "Currect Training Batch:  866\n",
            "Currect Training Batch:  867\n",
            "Currect Training Batch:  868\n",
            "Currect Training Batch:  869\n",
            "Currect Training Batch:  870\n",
            "Currect Training Batch:  871\n",
            "Currect Training Batch:  872\n",
            "Currect Training Batch:  873\n",
            "Currect Training Batch:  874\n",
            "Currect Training Batch:  875\n",
            "Currect Training Batch:  876\n",
            "Currect Training Batch:  877\n",
            "Currect Training Batch:  878\n",
            "Currect Training Batch:  879\n",
            "Currect Training Batch:  880\n",
            "Currect Training Batch:  881\n",
            "Currect Training Batch:  882\n",
            "Currect Training Batch:  883\n",
            "Currect Training Batch:  884\n",
            "Currect Training Batch:  885\n",
            "Currect Training Batch:  886\n",
            "Currect Training Batch:  887\n",
            "Currect Training Batch:  888\n",
            "Currect Training Batch:  889\n",
            "Currect Training Batch:  890\n",
            "Currect Training Batch:  891\n",
            "Currect Training Batch:  892\n",
            "Currect Training Batch:  893\n",
            "Currect Training Batch:  894\n",
            "Currect Training Batch:  895\n",
            "Currect Training Batch:  896\n",
            "Currect Training Batch:  897\n",
            "Currect Training Batch:  898\n",
            "Currect Training Batch:  899\n",
            "Currect Training Batch:  900\n",
            "Currect Training Batch:  901\n",
            "Currect Training Batch:  902\n",
            "Currect Training Batch:  903\n",
            "Currect Training Batch:  904\n",
            "Currect Training Batch:  905\n",
            "Currect Training Batch:  906\n",
            "Currect Training Batch:  907\n",
            "Currect Training Batch:  908\n",
            "Currect Training Batch:  909\n",
            "Currect Training Batch:  910\n",
            "Currect Training Batch:  911\n",
            "Currect Training Batch:  912\n",
            "Currect Training Batch:  913\n",
            "Currect Training Batch:  914\n",
            "Currect Training Batch:  915\n",
            "Currect Training Batch:  916\n",
            "Currect Training Batch:  917\n",
            "Currect Training Batch:  918\n",
            "Currect Training Batch:  919\n",
            "Currect Training Batch:  920\n",
            "Currect Training Batch:  921\n",
            "Currect Training Batch:  922\n",
            "Currect Training Batch:  923\n",
            "Currect Training Batch:  924\n",
            "Currect Training Batch:  925\n",
            "Currect Training Batch:  926\n",
            "Currect Training Batch:  927\n",
            "Current Validating Batch:  1\n",
            "Current Validating Batch:  2\n",
            "Current Validating Batch:  3\n",
            "Current Validating Batch:  4\n",
            "Current Validating Batch:  5\n",
            "Current Validating Batch:  6\n",
            "Current Validating Batch:  7\n",
            "Current Validating Batch:  8\n",
            "Current Validating Batch:  9\n",
            "Current Validating Batch:  10\n",
            "Current Validating Batch:  11\n",
            "Current Validating Batch:  12\n",
            "Current Validating Batch:  13\n",
            "Current Validating Batch:  14\n",
            "Current Validating Batch:  15\n",
            "Current Validating Batch:  16\n",
            "Current Validating Batch:  17\n",
            "Current Validating Batch:  18\n",
            "Current Validating Batch:  19\n",
            "Current Validating Batch:  20\n",
            "Current Validating Batch:  21\n",
            "Current Validating Batch:  22\n",
            "Current Validating Batch:  23\n",
            "Current Validating Batch:  24\n",
            "Current Validating Batch:  25\n",
            "Current Validating Batch:  26\n",
            "Current Validating Batch:  27\n",
            "Current Validating Batch:  28\n",
            "Current Validating Batch:  29\n",
            "Epoch: 9 \tTraining Loss: 0.072345 \tValidation Loss: 0.145029 \tAccuracy: 0.001082 \n",
            "Currect Training Batch:  928\n",
            "Currect Training Batch:  929\n",
            "Currect Training Batch:  930\n",
            "Currect Training Batch:  931\n",
            "Currect Training Batch:  932\n",
            "Currect Training Batch:  933\n",
            "Currect Training Batch:  934\n",
            "Currect Training Batch:  935\n",
            "Currect Training Batch:  936\n",
            "Currect Training Batch:  937\n",
            "Currect Training Batch:  938\n",
            "Currect Training Batch:  939\n",
            "Currect Training Batch:  940\n",
            "Currect Training Batch:  941\n",
            "Currect Training Batch:  942\n",
            "Currect Training Batch:  943\n",
            "Currect Training Batch:  944\n",
            "Currect Training Batch:  945\n",
            "Currect Training Batch:  946\n",
            "Currect Training Batch:  947\n",
            "Currect Training Batch:  948\n",
            "Currect Training Batch:  949\n",
            "Currect Training Batch:  950\n",
            "Currect Training Batch:  951\n",
            "Currect Training Batch:  952\n",
            "Currect Training Batch:  953\n",
            "Currect Training Batch:  954\n",
            "Currect Training Batch:  955\n",
            "Currect Training Batch:  956\n",
            "Currect Training Batch:  957\n",
            "Currect Training Batch:  958\n",
            "Currect Training Batch:  959\n",
            "Currect Training Batch:  960\n",
            "Currect Training Batch:  961\n",
            "Currect Training Batch:  962\n",
            "Currect Training Batch:  963\n",
            "Currect Training Batch:  964\n",
            "Currect Training Batch:  965\n",
            "Currect Training Batch:  966\n",
            "Currect Training Batch:  967\n",
            "Currect Training Batch:  968\n",
            "Currect Training Batch:  969\n",
            "Currect Training Batch:  970\n",
            "Currect Training Batch:  971\n",
            "Currect Training Batch:  972\n",
            "Currect Training Batch:  973\n",
            "Currect Training Batch:  974\n",
            "Currect Training Batch:  975\n",
            "Currect Training Batch:  976\n",
            "Currect Training Batch:  977\n",
            "Currect Training Batch:  978\n",
            "Currect Training Batch:  979\n",
            "Currect Training Batch:  980\n",
            "Currect Training Batch:  981\n",
            "Currect Training Batch:  982\n",
            "Currect Training Batch:  983\n",
            "Currect Training Batch:  984\n",
            "Currect Training Batch:  985\n",
            "Currect Training Batch:  986\n",
            "Currect Training Batch:  987\n",
            "Currect Training Batch:  988\n",
            "Currect Training Batch:  989\n",
            "Currect Training Batch:  990\n",
            "Currect Training Batch:  991\n",
            "Currect Training Batch:  992\n",
            "Currect Training Batch:  993\n",
            "Currect Training Batch:  994\n",
            "Currect Training Batch:  995\n",
            "Currect Training Batch:  996\n",
            "Currect Training Batch:  997\n",
            "Currect Training Batch:  998\n",
            "Currect Training Batch:  999\n",
            "Currect Training Batch:  1000\n",
            "Currect Training Batch:  1001\n",
            "Currect Training Batch:  1002\n",
            "Currect Training Batch:  1003\n",
            "Currect Training Batch:  1004\n",
            "Currect Training Batch:  1005\n",
            "Currect Training Batch:  1006\n",
            "Currect Training Batch:  1007\n",
            "Currect Training Batch:  1008\n",
            "Currect Training Batch:  1009\n",
            "Currect Training Batch:  1010\n",
            "Currect Training Batch:  1011\n",
            "Currect Training Batch:  1012\n",
            "Currect Training Batch:  1013\n",
            "Currect Training Batch:  1014\n",
            "Currect Training Batch:  1015\n",
            "Currect Training Batch:  1016\n",
            "Currect Training Batch:  1017\n",
            "Currect Training Batch:  1018\n",
            "Currect Training Batch:  1019\n",
            "Currect Training Batch:  1020\n",
            "Currect Training Batch:  1021\n",
            "Currect Training Batch:  1022\n",
            "Currect Training Batch:  1023\n",
            "Currect Training Batch:  1024\n",
            "Currect Training Batch:  1025\n",
            "Currect Training Batch:  1026\n",
            "Currect Training Batch:  1027\n",
            "Currect Training Batch:  1028\n",
            "Currect Training Batch:  1029\n",
            "Currect Training Batch:  1030\n",
            "Current Validating Batch:  1\n",
            "Current Validating Batch:  2\n",
            "Current Validating Batch:  3\n",
            "Current Validating Batch:  4\n",
            "Current Validating Batch:  5\n",
            "Current Validating Batch:  6\n",
            "Current Validating Batch:  7\n",
            "Current Validating Batch:  8\n",
            "Current Validating Batch:  9\n",
            "Current Validating Batch:  10\n",
            "Current Validating Batch:  11\n",
            "Current Validating Batch:  12\n",
            "Current Validating Batch:  13\n",
            "Current Validating Batch:  14\n",
            "Current Validating Batch:  15\n",
            "Current Validating Batch:  16\n",
            "Current Validating Batch:  17\n",
            "Current Validating Batch:  18\n",
            "Current Validating Batch:  19\n",
            "Current Validating Batch:  20\n",
            "Current Validating Batch:  21\n",
            "Current Validating Batch:  22\n",
            "Current Validating Batch:  23\n",
            "Current Validating Batch:  24\n",
            "Current Validating Batch:  25\n",
            "Current Validating Batch:  26\n",
            "Current Validating Batch:  27\n",
            "Current Validating Batch:  28\n",
            "Current Validating Batch:  29\n",
            "Epoch: 10 \tTraining Loss: 0.072340 \tValidation Loss: 0.145025 \tAccuracy: 0.001082 \n",
            "Currect Training Batch:  1031\n",
            "Currect Training Batch:  1032\n",
            "Currect Training Batch:  1033\n",
            "Currect Training Batch:  1034\n",
            "Currect Training Batch:  1035\n",
            "Currect Training Batch:  1036\n",
            "Currect Training Batch:  1037\n",
            "Currect Training Batch:  1038\n",
            "Currect Training Batch:  1039\n",
            "Currect Training Batch:  1040\n",
            "Currect Training Batch:  1041\n",
            "Currect Training Batch:  1042\n",
            "Currect Training Batch:  1043\n",
            "Currect Training Batch:  1044\n",
            "Currect Training Batch:  1045\n",
            "Currect Training Batch:  1046\n",
            "Currect Training Batch:  1047\n",
            "Currect Training Batch:  1048\n",
            "Currect Training Batch:  1049\n",
            "Currect Training Batch:  1050\n",
            "Currect Training Batch:  1051\n",
            "Currect Training Batch:  1052\n",
            "Currect Training Batch:  1053\n",
            "Currect Training Batch:  1054\n",
            "Currect Training Batch:  1055\n",
            "Currect Training Batch:  1056\n",
            "Currect Training Batch:  1057\n",
            "Currect Training Batch:  1058\n",
            "Currect Training Batch:  1059\n",
            "Currect Training Batch:  1060\n",
            "Currect Training Batch:  1061\n",
            "Currect Training Batch:  1062\n",
            "Currect Training Batch:  1063\n",
            "Currect Training Batch:  1064\n",
            "Currect Training Batch:  1065\n",
            "Currect Training Batch:  1066\n",
            "Currect Training Batch:  1067\n",
            "Currect Training Batch:  1068\n",
            "Currect Training Batch:  1069\n",
            "Currect Training Batch:  1070\n",
            "Currect Training Batch:  1071\n",
            "Currect Training Batch:  1072\n",
            "Currect Training Batch:  1073\n",
            "Currect Training Batch:  1074\n",
            "Currect Training Batch:  1075\n",
            "Currect Training Batch:  1076\n",
            "Currect Training Batch:  1077\n",
            "Currect Training Batch:  1078\n",
            "Currect Training Batch:  1079\n",
            "Currect Training Batch:  1080\n",
            "Currect Training Batch:  1081\n",
            "Currect Training Batch:  1082\n",
            "Currect Training Batch:  1083\n",
            "Currect Training Batch:  1084\n",
            "Currect Training Batch:  1085\n",
            "Currect Training Batch:  1086\n",
            "Currect Training Batch:  1087\n",
            "Currect Training Batch:  1088\n",
            "Currect Training Batch:  1089\n",
            "Currect Training Batch:  1090\n",
            "Currect Training Batch:  1091\n",
            "Currect Training Batch:  1092\n",
            "Currect Training Batch:  1093\n",
            "Currect Training Batch:  1094\n",
            "Currect Training Batch:  1095\n",
            "Currect Training Batch:  1096\n",
            "Currect Training Batch:  1097\n",
            "Currect Training Batch:  1098\n",
            "Currect Training Batch:  1099\n",
            "Currect Training Batch:  1100\n",
            "Currect Training Batch:  1101\n",
            "Currect Training Batch:  1102\n",
            "Currect Training Batch:  1103\n",
            "Currect Training Batch:  1104\n",
            "Currect Training Batch:  1105\n",
            "Currect Training Batch:  1106\n",
            "Currect Training Batch:  1107\n",
            "Currect Training Batch:  1108\n",
            "Currect Training Batch:  1109\n",
            "Currect Training Batch:  1110\n",
            "Currect Training Batch:  1111\n",
            "Currect Training Batch:  1112\n",
            "Currect Training Batch:  1113\n",
            "Currect Training Batch:  1114\n",
            "Currect Training Batch:  1115\n",
            "Currect Training Batch:  1116\n",
            "Currect Training Batch:  1117\n",
            "Currect Training Batch:  1118\n",
            "Currect Training Batch:  1119\n",
            "Currect Training Batch:  1120\n",
            "Currect Training Batch:  1121\n",
            "Currect Training Batch:  1122\n",
            "Currect Training Batch:  1123\n",
            "Currect Training Batch:  1124\n",
            "Currect Training Batch:  1125\n",
            "Currect Training Batch:  1126\n",
            "Currect Training Batch:  1127\n",
            "Currect Training Batch:  1128\n",
            "Currect Training Batch:  1129\n",
            "Currect Training Batch:  1130\n",
            "Currect Training Batch:  1131\n",
            "Currect Training Batch:  1132\n",
            "Currect Training Batch:  1133\n",
            "Current Validating Batch:  1\n",
            "Current Validating Batch:  2\n",
            "Current Validating Batch:  3\n",
            "Current Validating Batch:  4\n",
            "Current Validating Batch:  5\n",
            "Current Validating Batch:  6\n",
            "Current Validating Batch:  7\n",
            "Current Validating Batch:  8\n",
            "Current Validating Batch:  9\n",
            "Current Validating Batch:  10\n",
            "Current Validating Batch:  11\n",
            "Current Validating Batch:  12\n",
            "Current Validating Batch:  13\n",
            "Current Validating Batch:  14\n",
            "Current Validating Batch:  15\n",
            "Current Validating Batch:  16\n",
            "Current Validating Batch:  17\n",
            "Current Validating Batch:  18\n",
            "Current Validating Batch:  19\n",
            "Current Validating Batch:  20\n",
            "Current Validating Batch:  21\n",
            "Current Validating Batch:  22\n",
            "Current Validating Batch:  23\n",
            "Current Validating Batch:  24\n",
            "Current Validating Batch:  25\n",
            "Current Validating Batch:  26\n",
            "Current Validating Batch:  27\n",
            "Current Validating Batch:  28\n",
            "Current Validating Batch:  29\n",
            "Epoch: 11 \tTraining Loss: 0.072345 \tValidation Loss: 0.145035 \tAccuracy: 0.002165 \n",
            "Currect Training Batch:  1134\n",
            "Currect Training Batch:  1135\n",
            "Currect Training Batch:  1136\n",
            "Currect Training Batch:  1137\n",
            "Currect Training Batch:  1138\n",
            "Currect Training Batch:  1139\n",
            "Currect Training Batch:  1140\n",
            "Currect Training Batch:  1141\n",
            "Currect Training Batch:  1142\n",
            "Currect Training Batch:  1143\n",
            "Currect Training Batch:  1144\n",
            "Currect Training Batch:  1145\n",
            "Currect Training Batch:  1146\n",
            "Currect Training Batch:  1147\n",
            "Currect Training Batch:  1148\n",
            "Currect Training Batch:  1149\n",
            "Currect Training Batch:  1150\n",
            "Currect Training Batch:  1151\n",
            "Currect Training Batch:  1152\n",
            "Currect Training Batch:  1153\n",
            "Currect Training Batch:  1154\n",
            "Currect Training Batch:  1155\n",
            "Currect Training Batch:  1156\n",
            "Currect Training Batch:  1157\n",
            "Currect Training Batch:  1158\n",
            "Currect Training Batch:  1159\n",
            "Currect Training Batch:  1160\n",
            "Currect Training Batch:  1161\n",
            "Currect Training Batch:  1162\n",
            "Currect Training Batch:  1163\n",
            "Currect Training Batch:  1164\n",
            "Currect Training Batch:  1165\n",
            "Currect Training Batch:  1166\n",
            "Currect Training Batch:  1167\n",
            "Currect Training Batch:  1168\n",
            "Currect Training Batch:  1169\n",
            "Currect Training Batch:  1170\n",
            "Currect Training Batch:  1171\n",
            "Currect Training Batch:  1172\n",
            "Currect Training Batch:  1173\n",
            "Currect Training Batch:  1174\n",
            "Currect Training Batch:  1175\n",
            "Currect Training Batch:  1176\n",
            "Currect Training Batch:  1177\n",
            "Currect Training Batch:  1178\n",
            "Currect Training Batch:  1179\n",
            "Currect Training Batch:  1180\n",
            "Currect Training Batch:  1181\n",
            "Currect Training Batch:  1182\n",
            "Currect Training Batch:  1183\n",
            "Currect Training Batch:  1184\n",
            "Currect Training Batch:  1185\n",
            "Currect Training Batch:  1186\n",
            "Currect Training Batch:  1187\n",
            "Currect Training Batch:  1188\n",
            "Currect Training Batch:  1189\n",
            "Currect Training Batch:  1190\n",
            "Currect Training Batch:  1191\n",
            "Currect Training Batch:  1192\n",
            "Currect Training Batch:  1193\n",
            "Currect Training Batch:  1194\n",
            "Currect Training Batch:  1195\n",
            "Currect Training Batch:  1196\n",
            "Currect Training Batch:  1197\n",
            "Currect Training Batch:  1198\n",
            "Currect Training Batch:  1199\n",
            "Currect Training Batch:  1200\n",
            "Currect Training Batch:  1201\n",
            "Currect Training Batch:  1202\n",
            "Currect Training Batch:  1203\n",
            "Currect Training Batch:  1204\n",
            "Currect Training Batch:  1205\n",
            "Currect Training Batch:  1206\n",
            "Currect Training Batch:  1207\n",
            "Currect Training Batch:  1208\n",
            "Currect Training Batch:  1209\n",
            "Currect Training Batch:  1210\n",
            "Currect Training Batch:  1211\n",
            "Currect Training Batch:  1212\n",
            "Currect Training Batch:  1213\n",
            "Currect Training Batch:  1214\n",
            "Currect Training Batch:  1215\n",
            "Currect Training Batch:  1216\n",
            "Currect Training Batch:  1217\n",
            "Currect Training Batch:  1218\n",
            "Currect Training Batch:  1219\n",
            "Currect Training Batch:  1220\n",
            "Currect Training Batch:  1221\n",
            "Currect Training Batch:  1222\n",
            "Currect Training Batch:  1223\n",
            "Currect Training Batch:  1224\n",
            "Currect Training Batch:  1225\n",
            "Currect Training Batch:  1226\n",
            "Currect Training Batch:  1227\n",
            "Currect Training Batch:  1228\n",
            "Currect Training Batch:  1229\n",
            "Currect Training Batch:  1230\n",
            "Currect Training Batch:  1231\n",
            "Currect Training Batch:  1232\n",
            "Currect Training Batch:  1233\n",
            "Currect Training Batch:  1234\n",
            "Currect Training Batch:  1235\n",
            "Currect Training Batch:  1236\n",
            "Current Validating Batch:  1\n",
            "Current Validating Batch:  2\n",
            "Current Validating Batch:  3\n",
            "Current Validating Batch:  4\n",
            "Current Validating Batch:  5\n",
            "Current Validating Batch:  6\n",
            "Current Validating Batch:  7\n",
            "Current Validating Batch:  8\n",
            "Current Validating Batch:  9\n",
            "Current Validating Batch:  10\n",
            "Current Validating Batch:  11\n",
            "Current Validating Batch:  12\n",
            "Current Validating Batch:  13\n",
            "Current Validating Batch:  14\n",
            "Current Validating Batch:  15\n",
            "Current Validating Batch:  16\n",
            "Current Validating Batch:  17\n",
            "Current Validating Batch:  18\n",
            "Current Validating Batch:  19\n",
            "Current Validating Batch:  20\n",
            "Current Validating Batch:  21\n",
            "Current Validating Batch:  22\n",
            "Current Validating Batch:  23\n",
            "Current Validating Batch:  24\n",
            "Current Validating Batch:  25\n",
            "Current Validating Batch:  26\n",
            "Current Validating Batch:  27\n",
            "Current Validating Batch:  28\n",
            "Current Validating Batch:  29\n",
            "Epoch: 12 \tTraining Loss: 0.072347 \tValidation Loss: 0.145023 \tAccuracy: 0.002165 \n",
            "Currect Training Batch:  1237\n",
            "Currect Training Batch:  1238\n",
            "Currect Training Batch:  1239\n",
            "Currect Training Batch:  1240\n",
            "Currect Training Batch:  1241\n",
            "Currect Training Batch:  1242\n",
            "Currect Training Batch:  1243\n",
            "Currect Training Batch:  1244\n",
            "Currect Training Batch:  1245\n",
            "Currect Training Batch:  1246\n",
            "Currect Training Batch:  1247\n",
            "Currect Training Batch:  1248\n",
            "Currect Training Batch:  1249\n",
            "Currect Training Batch:  1250\n",
            "Currect Training Batch:  1251\n",
            "Currect Training Batch:  1252\n",
            "Currect Training Batch:  1253\n",
            "Currect Training Batch:  1254\n",
            "Currect Training Batch:  1255\n",
            "Currect Training Batch:  1256\n",
            "Currect Training Batch:  1257\n",
            "Currect Training Batch:  1258\n",
            "Currect Training Batch:  1259\n",
            "Currect Training Batch:  1260\n",
            "Currect Training Batch:  1261\n",
            "Currect Training Batch:  1262\n",
            "Currect Training Batch:  1263\n",
            "Currect Training Batch:  1264\n",
            "Currect Training Batch:  1265\n",
            "Currect Training Batch:  1266\n",
            "Currect Training Batch:  1267\n",
            "Currect Training Batch:  1268\n",
            "Currect Training Batch:  1269\n",
            "Currect Training Batch:  1270\n",
            "Currect Training Batch:  1271\n",
            "Currect Training Batch:  1272\n",
            "Currect Training Batch:  1273\n",
            "Currect Training Batch:  1274\n",
            "Currect Training Batch:  1275\n",
            "Currect Training Batch:  1276\n",
            "Currect Training Batch:  1277\n",
            "Currect Training Batch:  1278\n",
            "Currect Training Batch:  1279\n",
            "Currect Training Batch:  1280\n",
            "Currect Training Batch:  1281\n",
            "Currect Training Batch:  1282\n",
            "Currect Training Batch:  1283\n",
            "Currect Training Batch:  1284\n",
            "Currect Training Batch:  1285\n",
            "Currect Training Batch:  1286\n",
            "Currect Training Batch:  1287\n",
            "Currect Training Batch:  1288\n",
            "Currect Training Batch:  1289\n",
            "Currect Training Batch:  1290\n",
            "Currect Training Batch:  1291\n",
            "Currect Training Batch:  1292\n",
            "Currect Training Batch:  1293\n",
            "Currect Training Batch:  1294\n",
            "Currect Training Batch:  1295\n",
            "Currect Training Batch:  1296\n",
            "Currect Training Batch:  1297\n",
            "Currect Training Batch:  1298\n",
            "Currect Training Batch:  1299\n",
            "Currect Training Batch:  1300\n",
            "Currect Training Batch:  1301\n",
            "Currect Training Batch:  1302\n",
            "Currect Training Batch:  1303\n",
            "Currect Training Batch:  1304\n",
            "Currect Training Batch:  1305\n",
            "Currect Training Batch:  1306\n",
            "Currect Training Batch:  1307\n",
            "Currect Training Batch:  1308\n",
            "Currect Training Batch:  1309\n",
            "Currect Training Batch:  1310\n",
            "Currect Training Batch:  1311\n",
            "Currect Training Batch:  1312\n",
            "Currect Training Batch:  1313\n",
            "Currect Training Batch:  1314\n",
            "Currect Training Batch:  1315\n",
            "Currect Training Batch:  1316\n",
            "Currect Training Batch:  1317\n",
            "Currect Training Batch:  1318\n",
            "Currect Training Batch:  1319\n",
            "Currect Training Batch:  1320\n",
            "Currect Training Batch:  1321\n",
            "Currect Training Batch:  1322\n",
            "Currect Training Batch:  1323\n",
            "Currect Training Batch:  1324\n",
            "Currect Training Batch:  1325\n",
            "Currect Training Batch:  1326\n",
            "Currect Training Batch:  1327\n",
            "Currect Training Batch:  1328\n",
            "Currect Training Batch:  1329\n",
            "Currect Training Batch:  1330\n",
            "Currect Training Batch:  1331\n",
            "Currect Training Batch:  1332\n",
            "Currect Training Batch:  1333\n",
            "Currect Training Batch:  1334\n",
            "Currect Training Batch:  1335\n",
            "Currect Training Batch:  1336\n",
            "Currect Training Batch:  1337\n",
            "Currect Training Batch:  1338\n",
            "Currect Training Batch:  1339\n",
            "Current Validating Batch:  1\n",
            "Current Validating Batch:  2\n",
            "Current Validating Batch:  3\n",
            "Current Validating Batch:  4\n",
            "Current Validating Batch:  5\n",
            "Current Validating Batch:  6\n",
            "Current Validating Batch:  7\n",
            "Current Validating Batch:  8\n",
            "Current Validating Batch:  9\n",
            "Current Validating Batch:  10\n",
            "Current Validating Batch:  11\n",
            "Current Validating Batch:  12\n",
            "Current Validating Batch:  13\n",
            "Current Validating Batch:  14\n",
            "Current Validating Batch:  15\n",
            "Current Validating Batch:  16\n",
            "Current Validating Batch:  17\n",
            "Current Validating Batch:  18\n",
            "Current Validating Batch:  19\n",
            "Current Validating Batch:  20\n",
            "Current Validating Batch:  21\n",
            "Current Validating Batch:  22\n",
            "Current Validating Batch:  23\n",
            "Current Validating Batch:  24\n",
            "Current Validating Batch:  25\n",
            "Current Validating Batch:  26\n",
            "Current Validating Batch:  27\n",
            "Current Validating Batch:  28\n",
            "Current Validating Batch:  29\n",
            "Epoch: 13 \tTraining Loss: 0.072334 \tValidation Loss: 0.145015 \tAccuracy: 0.004329 \n",
            "Validation loss decreased (0.145016 --> 0.145015).  Saving model ...\n",
            "Currect Training Batch:  1340\n",
            "Currect Training Batch:  1341\n",
            "Currect Training Batch:  1342\n",
            "Currect Training Batch:  1343\n",
            "Currect Training Batch:  1344\n",
            "Currect Training Batch:  1345\n",
            "Currect Training Batch:  1346\n",
            "Currect Training Batch:  1347\n",
            "Currect Training Batch:  1348\n",
            "Currect Training Batch:  1349\n",
            "Currect Training Batch:  1350\n",
            "Currect Training Batch:  1351\n",
            "Currect Training Batch:  1352\n",
            "Currect Training Batch:  1353\n",
            "Currect Training Batch:  1354\n",
            "Currect Training Batch:  1355\n",
            "Currect Training Batch:  1356\n",
            "Currect Training Batch:  1357\n",
            "Currect Training Batch:  1358\n",
            "Currect Training Batch:  1359\n",
            "Currect Training Batch:  1360\n",
            "Currect Training Batch:  1361\n",
            "Currect Training Batch:  1362\n",
            "Currect Training Batch:  1363\n",
            "Currect Training Batch:  1364\n",
            "Currect Training Batch:  1365\n",
            "Currect Training Batch:  1366\n",
            "Currect Training Batch:  1367\n",
            "Currect Training Batch:  1368\n",
            "Currect Training Batch:  1369\n",
            "Currect Training Batch:  1370\n",
            "Currect Training Batch:  1371\n",
            "Currect Training Batch:  1372\n",
            "Currect Training Batch:  1373\n",
            "Currect Training Batch:  1374\n",
            "Currect Training Batch:  1375\n",
            "Currect Training Batch:  1376\n",
            "Currect Training Batch:  1377\n",
            "Currect Training Batch:  1378\n",
            "Currect Training Batch:  1379\n",
            "Currect Training Batch:  1380\n",
            "Currect Training Batch:  1381\n",
            "Currect Training Batch:  1382\n",
            "Currect Training Batch:  1383\n",
            "Currect Training Batch:  1384\n",
            "Currect Training Batch:  1385\n",
            "Currect Training Batch:  1386\n",
            "Currect Training Batch:  1387\n",
            "Currect Training Batch:  1388\n",
            "Currect Training Batch:  1389\n",
            "Currect Training Batch:  1390\n",
            "Currect Training Batch:  1391\n",
            "Currect Training Batch:  1392\n",
            "Currect Training Batch:  1393\n",
            "Currect Training Batch:  1394\n",
            "Currect Training Batch:  1395\n",
            "Currect Training Batch:  1396\n",
            "Currect Training Batch:  1397\n",
            "Currect Training Batch:  1398\n",
            "Currect Training Batch:  1399\n",
            "Currect Training Batch:  1400\n",
            "Currect Training Batch:  1401\n",
            "Currect Training Batch:  1402\n",
            "Currect Training Batch:  1403\n",
            "Currect Training Batch:  1404\n",
            "Currect Training Batch:  1405\n",
            "Currect Training Batch:  1406\n",
            "Currect Training Batch:  1407\n",
            "Currect Training Batch:  1408\n",
            "Currect Training Batch:  1409\n",
            "Currect Training Batch:  1410\n",
            "Currect Training Batch:  1411\n",
            "Currect Training Batch:  1412\n",
            "Currect Training Batch:  1413\n",
            "Currect Training Batch:  1414\n",
            "Currect Training Batch:  1415\n",
            "Currect Training Batch:  1416\n",
            "Currect Training Batch:  1417\n",
            "Currect Training Batch:  1418\n",
            "Currect Training Batch:  1419\n",
            "Currect Training Batch:  1420\n",
            "Currect Training Batch:  1421\n",
            "Currect Training Batch:  1422\n",
            "Currect Training Batch:  1423\n",
            "Currect Training Batch:  1424\n",
            "Currect Training Batch:  1425\n",
            "Currect Training Batch:  1426\n",
            "Currect Training Batch:  1427\n",
            "Currect Training Batch:  1428\n",
            "Currect Training Batch:  1429\n",
            "Currect Training Batch:  1430\n",
            "Currect Training Batch:  1431\n",
            "Currect Training Batch:  1432\n",
            "Currect Training Batch:  1433\n",
            "Currect Training Batch:  1434\n",
            "Currect Training Batch:  1435\n",
            "Currect Training Batch:  1436\n",
            "Currect Training Batch:  1437\n",
            "Currect Training Batch:  1438\n",
            "Currect Training Batch:  1439\n",
            "Currect Training Batch:  1440\n",
            "Currect Training Batch:  1441\n",
            "Currect Training Batch:  1442\n",
            "Current Validating Batch:  1\n",
            "Current Validating Batch:  2\n",
            "Current Validating Batch:  3\n",
            "Current Validating Batch:  4\n",
            "Current Validating Batch:  5\n",
            "Current Validating Batch:  6\n",
            "Current Validating Batch:  7\n",
            "Current Validating Batch:  8\n",
            "Current Validating Batch:  9\n",
            "Current Validating Batch:  10\n",
            "Current Validating Batch:  11\n",
            "Current Validating Batch:  12\n",
            "Current Validating Batch:  13\n",
            "Current Validating Batch:  14\n",
            "Current Validating Batch:  15\n",
            "Current Validating Batch:  16\n",
            "Current Validating Batch:  17\n",
            "Current Validating Batch:  18\n",
            "Current Validating Batch:  19\n",
            "Current Validating Batch:  20\n",
            "Current Validating Batch:  21\n",
            "Current Validating Batch:  22\n",
            "Current Validating Batch:  23\n",
            "Current Validating Batch:  24\n",
            "Current Validating Batch:  25\n",
            "Current Validating Batch:  26\n",
            "Current Validating Batch:  27\n",
            "Current Validating Batch:  28\n",
            "Current Validating Batch:  29\n",
            "Epoch: 14 \tTraining Loss: 0.072334 \tValidation Loss: 0.145026 \tAccuracy: 0.001082 \n",
            "Currect Training Batch:  1443\n",
            "Currect Training Batch:  1444\n",
            "Currect Training Batch:  1445\n",
            "Currect Training Batch:  1446\n",
            "Currect Training Batch:  1447\n",
            "Currect Training Batch:  1448\n",
            "Currect Training Batch:  1449\n",
            "Currect Training Batch:  1450\n",
            "Currect Training Batch:  1451\n",
            "Currect Training Batch:  1452\n",
            "Currect Training Batch:  1453\n",
            "Currect Training Batch:  1454\n",
            "Currect Training Batch:  1455\n",
            "Currect Training Batch:  1456\n",
            "Currect Training Batch:  1457\n",
            "Currect Training Batch:  1458\n",
            "Currect Training Batch:  1459\n",
            "Currect Training Batch:  1460\n",
            "Currect Training Batch:  1461\n",
            "Currect Training Batch:  1462\n",
            "Currect Training Batch:  1463\n",
            "Currect Training Batch:  1464\n",
            "Currect Training Batch:  1465\n",
            "Currect Training Batch:  1466\n",
            "Currect Training Batch:  1467\n",
            "Currect Training Batch:  1468\n",
            "Currect Training Batch:  1469\n",
            "Currect Training Batch:  1470\n",
            "Currect Training Batch:  1471\n",
            "Currect Training Batch:  1472\n",
            "Currect Training Batch:  1473\n",
            "Currect Training Batch:  1474\n",
            "Currect Training Batch:  1475\n",
            "Currect Training Batch:  1476\n",
            "Currect Training Batch:  1477\n",
            "Currect Training Batch:  1478\n",
            "Currect Training Batch:  1479\n",
            "Currect Training Batch:  1480\n",
            "Currect Training Batch:  1481\n",
            "Currect Training Batch:  1482\n",
            "Currect Training Batch:  1483\n",
            "Currect Training Batch:  1484\n",
            "Currect Training Batch:  1485\n",
            "Currect Training Batch:  1486\n",
            "Currect Training Batch:  1487\n",
            "Currect Training Batch:  1488\n",
            "Currect Training Batch:  1489\n",
            "Currect Training Batch:  1490\n",
            "Currect Training Batch:  1491\n",
            "Currect Training Batch:  1492\n",
            "Currect Training Batch:  1493\n",
            "Currect Training Batch:  1494\n",
            "Currect Training Batch:  1495\n",
            "Currect Training Batch:  1496\n",
            "Currect Training Batch:  1497\n",
            "Currect Training Batch:  1498\n",
            "Currect Training Batch:  1499\n",
            "Currect Training Batch:  1500\n",
            "Currect Training Batch:  1501\n",
            "Currect Training Batch:  1502\n",
            "Currect Training Batch:  1503\n",
            "Currect Training Batch:  1504\n",
            "Currect Training Batch:  1505\n",
            "Currect Training Batch:  1506\n",
            "Currect Training Batch:  1507\n",
            "Currect Training Batch:  1508\n",
            "Currect Training Batch:  1509\n",
            "Currect Training Batch:  1510\n",
            "Currect Training Batch:  1511\n",
            "Currect Training Batch:  1512\n",
            "Currect Training Batch:  1513\n",
            "Currect Training Batch:  1514\n",
            "Currect Training Batch:  1515\n",
            "Currect Training Batch:  1516\n",
            "Currect Training Batch:  1517\n",
            "Currect Training Batch:  1518\n",
            "Currect Training Batch:  1519\n",
            "Currect Training Batch:  1520\n",
            "Currect Training Batch:  1521\n",
            "Currect Training Batch:  1522\n",
            "Currect Training Batch:  1523\n",
            "Currect Training Batch:  1524\n",
            "Currect Training Batch:  1525\n",
            "Currect Training Batch:  1526\n",
            "Currect Training Batch:  1527\n",
            "Currect Training Batch:  1528\n",
            "Currect Training Batch:  1529\n",
            "Currect Training Batch:  1530\n",
            "Currect Training Batch:  1531\n",
            "Currect Training Batch:  1532\n",
            "Currect Training Batch:  1533\n",
            "Currect Training Batch:  1534\n",
            "Currect Training Batch:  1535\n",
            "Currect Training Batch:  1536\n",
            "Currect Training Batch:  1537\n",
            "Currect Training Batch:  1538\n",
            "Currect Training Batch:  1539\n",
            "Currect Training Batch:  1540\n",
            "Currect Training Batch:  1541\n",
            "Currect Training Batch:  1542\n",
            "Currect Training Batch:  1543\n",
            "Currect Training Batch:  1544\n",
            "Currect Training Batch:  1545\n",
            "Current Validating Batch:  1\n",
            "Current Validating Batch:  2\n",
            "Current Validating Batch:  3\n",
            "Current Validating Batch:  4\n",
            "Current Validating Batch:  5\n",
            "Current Validating Batch:  6\n",
            "Current Validating Batch:  7\n",
            "Current Validating Batch:  8\n",
            "Current Validating Batch:  9\n",
            "Current Validating Batch:  10\n",
            "Current Validating Batch:  11\n",
            "Current Validating Batch:  12\n",
            "Current Validating Batch:  13\n",
            "Current Validating Batch:  14\n",
            "Current Validating Batch:  15\n",
            "Current Validating Batch:  16\n",
            "Current Validating Batch:  17\n",
            "Current Validating Batch:  18\n",
            "Current Validating Batch:  19\n",
            "Current Validating Batch:  20\n",
            "Current Validating Batch:  21\n",
            "Current Validating Batch:  22\n",
            "Current Validating Batch:  23\n",
            "Current Validating Batch:  24\n",
            "Current Validating Batch:  25\n",
            "Current Validating Batch:  26\n",
            "Current Validating Batch:  27\n",
            "Current Validating Batch:  28\n",
            "Current Validating Batch:  29\n",
            "Epoch: 15 \tTraining Loss: 0.072346 \tValidation Loss: 0.145017 \tAccuracy: 0.001082 \n",
            "Currect Training Batch:  1546\n",
            "Currect Training Batch:  1547\n",
            "Currect Training Batch:  1548\n",
            "Currect Training Batch:  1549\n",
            "Currect Training Batch:  1550\n",
            "Currect Training Batch:  1551\n",
            "Currect Training Batch:  1552\n",
            "Currect Training Batch:  1553\n",
            "Currect Training Batch:  1554\n",
            "Currect Training Batch:  1555\n",
            "Currect Training Batch:  1556\n",
            "Currect Training Batch:  1557\n",
            "Currect Training Batch:  1558\n",
            "Currect Training Batch:  1559\n",
            "Currect Training Batch:  1560\n",
            "Currect Training Batch:  1561\n",
            "Currect Training Batch:  1562\n",
            "Currect Training Batch:  1563\n",
            "Currect Training Batch:  1564\n",
            "Currect Training Batch:  1565\n",
            "Currect Training Batch:  1566\n",
            "Currect Training Batch:  1567\n",
            "Currect Training Batch:  1568\n",
            "Currect Training Batch:  1569\n",
            "Currect Training Batch:  1570\n",
            "Currect Training Batch:  1571\n",
            "Currect Training Batch:  1572\n",
            "Currect Training Batch:  1573\n",
            "Currect Training Batch:  1574\n",
            "Currect Training Batch:  1575\n",
            "Currect Training Batch:  1576\n",
            "Currect Training Batch:  1577\n",
            "Currect Training Batch:  1578\n",
            "Currect Training Batch:  1579\n",
            "Currect Training Batch:  1580\n",
            "Currect Training Batch:  1581\n",
            "Currect Training Batch:  1582\n",
            "Currect Training Batch:  1583\n",
            "Currect Training Batch:  1584\n",
            "Currect Training Batch:  1585\n",
            "Currect Training Batch:  1586\n",
            "Currect Training Batch:  1587\n",
            "Currect Training Batch:  1588\n",
            "Currect Training Batch:  1589\n",
            "Currect Training Batch:  1590\n",
            "Currect Training Batch:  1591\n",
            "Currect Training Batch:  1592\n",
            "Currect Training Batch:  1593\n",
            "Currect Training Batch:  1594\n",
            "Currect Training Batch:  1595\n",
            "Currect Training Batch:  1596\n",
            "Currect Training Batch:  1597\n",
            "Currect Training Batch:  1598\n",
            "Currect Training Batch:  1599\n",
            "Currect Training Batch:  1600\n",
            "Currect Training Batch:  1601\n",
            "Currect Training Batch:  1602\n",
            "Currect Training Batch:  1603\n",
            "Currect Training Batch:  1604\n",
            "Currect Training Batch:  1605\n",
            "Currect Training Batch:  1606\n",
            "Currect Training Batch:  1607\n",
            "Currect Training Batch:  1608\n",
            "Currect Training Batch:  1609\n",
            "Currect Training Batch:  1610\n",
            "Currect Training Batch:  1611\n",
            "Currect Training Batch:  1612\n",
            "Currect Training Batch:  1613\n",
            "Currect Training Batch:  1614\n",
            "Currect Training Batch:  1615\n",
            "Currect Training Batch:  1616\n",
            "Currect Training Batch:  1617\n",
            "Currect Training Batch:  1618\n",
            "Currect Training Batch:  1619\n",
            "Currect Training Batch:  1620\n",
            "Currect Training Batch:  1621\n",
            "Currect Training Batch:  1622\n",
            "Currect Training Batch:  1623\n",
            "Currect Training Batch:  1624\n",
            "Currect Training Batch:  1625\n",
            "Currect Training Batch:  1626\n",
            "Currect Training Batch:  1627\n",
            "Currect Training Batch:  1628\n",
            "Currect Training Batch:  1629\n",
            "Currect Training Batch:  1630\n",
            "Currect Training Batch:  1631\n",
            "Currect Training Batch:  1632\n",
            "Currect Training Batch:  1633\n",
            "Currect Training Batch:  1634\n",
            "Currect Training Batch:  1635\n",
            "Currect Training Batch:  1636\n",
            "Currect Training Batch:  1637\n",
            "Currect Training Batch:  1638\n",
            "Currect Training Batch:  1639\n",
            "Currect Training Batch:  1640\n",
            "Currect Training Batch:  1641\n",
            "Currect Training Batch:  1642\n",
            "Currect Training Batch:  1643\n",
            "Currect Training Batch:  1644\n",
            "Currect Training Batch:  1645\n",
            "Currect Training Batch:  1646\n",
            "Currect Training Batch:  1647\n",
            "Currect Training Batch:  1648\n",
            "Current Validating Batch:  1\n",
            "Current Validating Batch:  2\n",
            "Current Validating Batch:  3\n",
            "Current Validating Batch:  4\n",
            "Current Validating Batch:  5\n",
            "Current Validating Batch:  6\n",
            "Current Validating Batch:  7\n",
            "Current Validating Batch:  8\n",
            "Current Validating Batch:  9\n",
            "Current Validating Batch:  10\n",
            "Current Validating Batch:  11\n",
            "Current Validating Batch:  12\n",
            "Current Validating Batch:  13\n",
            "Current Validating Batch:  14\n",
            "Current Validating Batch:  15\n",
            "Current Validating Batch:  16\n",
            "Current Validating Batch:  17\n",
            "Current Validating Batch:  18\n",
            "Current Validating Batch:  19\n",
            "Current Validating Batch:  20\n",
            "Current Validating Batch:  21\n",
            "Current Validating Batch:  22\n",
            "Current Validating Batch:  23\n",
            "Current Validating Batch:  24\n",
            "Current Validating Batch:  25\n",
            "Current Validating Batch:  26\n",
            "Current Validating Batch:  27\n",
            "Current Validating Batch:  28\n",
            "Current Validating Batch:  29\n",
            "Epoch: 16 \tTraining Loss: 0.072338 \tValidation Loss: 0.145019 \tAccuracy: 0.003247 \n",
            "Currect Training Batch:  1649\n",
            "Currect Training Batch:  1650\n",
            "Currect Training Batch:  1651\n",
            "Currect Training Batch:  1652\n",
            "Currect Training Batch:  1653\n",
            "Currect Training Batch:  1654\n",
            "Currect Training Batch:  1655\n",
            "Currect Training Batch:  1656\n",
            "Currect Training Batch:  1657\n",
            "Currect Training Batch:  1658\n",
            "Currect Training Batch:  1659\n",
            "Currect Training Batch:  1660\n",
            "Currect Training Batch:  1661\n",
            "Currect Training Batch:  1662\n",
            "Currect Training Batch:  1663\n",
            "Currect Training Batch:  1664\n",
            "Currect Training Batch:  1665\n",
            "Currect Training Batch:  1666\n",
            "Currect Training Batch:  1667\n",
            "Currect Training Batch:  1668\n",
            "Currect Training Batch:  1669\n",
            "Currect Training Batch:  1670\n",
            "Currect Training Batch:  1671\n",
            "Currect Training Batch:  1672\n",
            "Currect Training Batch:  1673\n",
            "Currect Training Batch:  1674\n",
            "Currect Training Batch:  1675\n",
            "Currect Training Batch:  1676\n",
            "Currect Training Batch:  1677\n",
            "Currect Training Batch:  1678\n",
            "Currect Training Batch:  1679\n",
            "Currect Training Batch:  1680\n",
            "Currect Training Batch:  1681\n",
            "Currect Training Batch:  1682\n",
            "Currect Training Batch:  1683\n",
            "Currect Training Batch:  1684\n",
            "Currect Training Batch:  1685\n",
            "Currect Training Batch:  1686\n",
            "Currect Training Batch:  1687\n",
            "Currect Training Batch:  1688\n",
            "Currect Training Batch:  1689\n",
            "Currect Training Batch:  1690\n",
            "Currect Training Batch:  1691\n",
            "Currect Training Batch:  1692\n",
            "Currect Training Batch:  1693\n",
            "Currect Training Batch:  1694\n",
            "Currect Training Batch:  1695\n",
            "Currect Training Batch:  1696\n",
            "Currect Training Batch:  1697\n",
            "Currect Training Batch:  1698\n",
            "Currect Training Batch:  1699\n",
            "Currect Training Batch:  1700\n",
            "Currect Training Batch:  1701\n",
            "Currect Training Batch:  1702\n",
            "Currect Training Batch:  1703\n",
            "Currect Training Batch:  1704\n",
            "Currect Training Batch:  1705\n",
            "Currect Training Batch:  1706\n",
            "Currect Training Batch:  1707\n",
            "Currect Training Batch:  1708\n",
            "Currect Training Batch:  1709\n",
            "Currect Training Batch:  1710\n",
            "Currect Training Batch:  1711\n",
            "Currect Training Batch:  1712\n",
            "Currect Training Batch:  1713\n",
            "Currect Training Batch:  1714\n",
            "Currect Training Batch:  1715\n",
            "Currect Training Batch:  1716\n",
            "Currect Training Batch:  1717\n",
            "Currect Training Batch:  1718\n",
            "Currect Training Batch:  1719\n",
            "Currect Training Batch:  1720\n",
            "Currect Training Batch:  1721\n",
            "Currect Training Batch:  1722\n",
            "Currect Training Batch:  1723\n",
            "Currect Training Batch:  1724\n",
            "Currect Training Batch:  1725\n",
            "Currect Training Batch:  1726\n",
            "Currect Training Batch:  1727\n",
            "Currect Training Batch:  1728\n",
            "Currect Training Batch:  1729\n",
            "Currect Training Batch:  1730\n",
            "Currect Training Batch:  1731\n",
            "Currect Training Batch:  1732\n",
            "Currect Training Batch:  1733\n",
            "Currect Training Batch:  1734\n",
            "Currect Training Batch:  1735\n",
            "Currect Training Batch:  1736\n",
            "Currect Training Batch:  1737\n",
            "Currect Training Batch:  1738\n",
            "Currect Training Batch:  1739\n",
            "Currect Training Batch:  1740\n",
            "Currect Training Batch:  1741\n",
            "Currect Training Batch:  1742\n",
            "Currect Training Batch:  1743\n",
            "Currect Training Batch:  1744\n",
            "Currect Training Batch:  1745\n",
            "Currect Training Batch:  1746\n",
            "Currect Training Batch:  1747\n",
            "Currect Training Batch:  1748\n",
            "Currect Training Batch:  1749\n",
            "Currect Training Batch:  1750\n",
            "Currect Training Batch:  1751\n",
            "Current Validating Batch:  1\n",
            "Current Validating Batch:  2\n",
            "Current Validating Batch:  3\n",
            "Current Validating Batch:  4\n",
            "Current Validating Batch:  5\n",
            "Current Validating Batch:  6\n",
            "Current Validating Batch:  7\n",
            "Current Validating Batch:  8\n",
            "Current Validating Batch:  9\n",
            "Current Validating Batch:  10\n",
            "Current Validating Batch:  11\n",
            "Current Validating Batch:  12\n",
            "Current Validating Batch:  13\n",
            "Current Validating Batch:  14\n",
            "Current Validating Batch:  15\n",
            "Current Validating Batch:  16\n",
            "Current Validating Batch:  17\n",
            "Current Validating Batch:  18\n",
            "Current Validating Batch:  19\n",
            "Current Validating Batch:  20\n",
            "Current Validating Batch:  21\n",
            "Current Validating Batch:  22\n",
            "Current Validating Batch:  23\n",
            "Current Validating Batch:  24\n",
            "Current Validating Batch:  25\n",
            "Current Validating Batch:  26\n",
            "Current Validating Batch:  27\n",
            "Current Validating Batch:  28\n",
            "Current Validating Batch:  29\n",
            "Epoch: 17 \tTraining Loss: 0.072337 \tValidation Loss: 0.145006 \tAccuracy: 0.004329 \n",
            "Validation loss decreased (0.145015 --> 0.145006).  Saving model ...\n",
            "Currect Training Batch:  1752\n",
            "Currect Training Batch:  1753\n",
            "Currect Training Batch:  1754\n",
            "Currect Training Batch:  1755\n",
            "Currect Training Batch:  1756\n",
            "Currect Training Batch:  1757\n",
            "Currect Training Batch:  1758\n",
            "Currect Training Batch:  1759\n",
            "Currect Training Batch:  1760\n",
            "Currect Training Batch:  1761\n",
            "Currect Training Batch:  1762\n",
            "Currect Training Batch:  1763\n",
            "Currect Training Batch:  1764\n",
            "Currect Training Batch:  1765\n",
            "Currect Training Batch:  1766\n",
            "Currect Training Batch:  1767\n",
            "Currect Training Batch:  1768\n",
            "Currect Training Batch:  1769\n",
            "Currect Training Batch:  1770\n",
            "Currect Training Batch:  1771\n",
            "Currect Training Batch:  1772\n",
            "Currect Training Batch:  1773\n",
            "Currect Training Batch:  1774\n",
            "Currect Training Batch:  1775\n",
            "Currect Training Batch:  1776\n",
            "Currect Training Batch:  1777\n",
            "Currect Training Batch:  1778\n",
            "Currect Training Batch:  1779\n",
            "Currect Training Batch:  1780\n",
            "Currect Training Batch:  1781\n",
            "Currect Training Batch:  1782\n",
            "Currect Training Batch:  1783\n",
            "Currect Training Batch:  1784\n",
            "Currect Training Batch:  1785\n",
            "Currect Training Batch:  1786\n",
            "Currect Training Batch:  1787\n",
            "Currect Training Batch:  1788\n",
            "Currect Training Batch:  1789\n",
            "Currect Training Batch:  1790\n",
            "Currect Training Batch:  1791\n",
            "Currect Training Batch:  1792\n",
            "Currect Training Batch:  1793\n",
            "Currect Training Batch:  1794\n",
            "Currect Training Batch:  1795\n",
            "Currect Training Batch:  1796\n",
            "Currect Training Batch:  1797\n",
            "Currect Training Batch:  1798\n",
            "Currect Training Batch:  1799\n",
            "Currect Training Batch:  1800\n",
            "Currect Training Batch:  1801\n",
            "Currect Training Batch:  1802\n",
            "Currect Training Batch:  1803\n",
            "Currect Training Batch:  1804\n",
            "Currect Training Batch:  1805\n",
            "Currect Training Batch:  1806\n",
            "Currect Training Batch:  1807\n",
            "Currect Training Batch:  1808\n",
            "Currect Training Batch:  1809\n",
            "Currect Training Batch:  1810\n",
            "Currect Training Batch:  1811\n",
            "Currect Training Batch:  1812\n",
            "Currect Training Batch:  1813\n",
            "Currect Training Batch:  1814\n",
            "Currect Training Batch:  1815\n",
            "Currect Training Batch:  1816\n",
            "Currect Training Batch:  1817\n",
            "Currect Training Batch:  1818\n",
            "Currect Training Batch:  1819\n",
            "Currect Training Batch:  1820\n",
            "Currect Training Batch:  1821\n",
            "Currect Training Batch:  1822\n",
            "Currect Training Batch:  1823\n",
            "Currect Training Batch:  1824\n",
            "Currect Training Batch:  1825\n",
            "Currect Training Batch:  1826\n",
            "Currect Training Batch:  1827\n",
            "Currect Training Batch:  1828\n",
            "Currect Training Batch:  1829\n",
            "Currect Training Batch:  1830\n",
            "Currect Training Batch:  1831\n",
            "Currect Training Batch:  1832\n",
            "Currect Training Batch:  1833\n",
            "Currect Training Batch:  1834\n",
            "Currect Training Batch:  1835\n",
            "Currect Training Batch:  1836\n",
            "Currect Training Batch:  1837\n",
            "Currect Training Batch:  1838\n",
            "Currect Training Batch:  1839\n",
            "Currect Training Batch:  1840\n",
            "Currect Training Batch:  1841\n",
            "Currect Training Batch:  1842\n",
            "Currect Training Batch:  1843\n",
            "Currect Training Batch:  1844\n",
            "Currect Training Batch:  1845\n",
            "Currect Training Batch:  1846\n",
            "Currect Training Batch:  1847\n",
            "Currect Training Batch:  1848\n",
            "Currect Training Batch:  1849\n",
            "Currect Training Batch:  1850\n",
            "Currect Training Batch:  1851\n",
            "Currect Training Batch:  1852\n",
            "Currect Training Batch:  1853\n",
            "Currect Training Batch:  1854\n",
            "Current Validating Batch:  1\n",
            "Current Validating Batch:  2\n",
            "Current Validating Batch:  3\n",
            "Current Validating Batch:  4\n",
            "Current Validating Batch:  5\n",
            "Current Validating Batch:  6\n",
            "Current Validating Batch:  7\n",
            "Current Validating Batch:  8\n",
            "Current Validating Batch:  9\n",
            "Current Validating Batch:  10\n",
            "Current Validating Batch:  11\n",
            "Current Validating Batch:  12\n",
            "Current Validating Batch:  13\n",
            "Current Validating Batch:  14\n",
            "Current Validating Batch:  15\n",
            "Current Validating Batch:  16\n",
            "Current Validating Batch:  17\n",
            "Current Validating Batch:  18\n",
            "Current Validating Batch:  19\n",
            "Current Validating Batch:  20\n",
            "Current Validating Batch:  21\n",
            "Current Validating Batch:  22\n",
            "Current Validating Batch:  23\n",
            "Current Validating Batch:  24\n",
            "Current Validating Batch:  25\n",
            "Current Validating Batch:  26\n",
            "Current Validating Batch:  27\n",
            "Current Validating Batch:  28\n",
            "Current Validating Batch:  29\n",
            "Epoch: 18 \tTraining Loss: 0.072331 \tValidation Loss: 0.145022 \tAccuracy: 0.002165 \n",
            "Currect Training Batch:  1855\n",
            "Currect Training Batch:  1856\n",
            "Currect Training Batch:  1857\n",
            "Currect Training Batch:  1858\n",
            "Currect Training Batch:  1859\n",
            "Currect Training Batch:  1860\n",
            "Currect Training Batch:  1861\n",
            "Currect Training Batch:  1862\n",
            "Currect Training Batch:  1863\n",
            "Currect Training Batch:  1864\n",
            "Currect Training Batch:  1865\n",
            "Currect Training Batch:  1866\n",
            "Currect Training Batch:  1867\n",
            "Currect Training Batch:  1868\n",
            "Currect Training Batch:  1869\n",
            "Currect Training Batch:  1870\n",
            "Currect Training Batch:  1871\n",
            "Currect Training Batch:  1872\n",
            "Currect Training Batch:  1873\n",
            "Currect Training Batch:  1874\n",
            "Currect Training Batch:  1875\n",
            "Currect Training Batch:  1876\n",
            "Currect Training Batch:  1877\n",
            "Currect Training Batch:  1878\n",
            "Currect Training Batch:  1879\n",
            "Currect Training Batch:  1880\n",
            "Currect Training Batch:  1881\n",
            "Currect Training Batch:  1882\n",
            "Currect Training Batch:  1883\n",
            "Currect Training Batch:  1884\n",
            "Currect Training Batch:  1885\n",
            "Currect Training Batch:  1886\n",
            "Currect Training Batch:  1887\n",
            "Currect Training Batch:  1888\n",
            "Currect Training Batch:  1889\n",
            "Currect Training Batch:  1890\n",
            "Currect Training Batch:  1891\n",
            "Currect Training Batch:  1892\n",
            "Currect Training Batch:  1893\n",
            "Currect Training Batch:  1894\n",
            "Currect Training Batch:  1895\n",
            "Currect Training Batch:  1896\n",
            "Currect Training Batch:  1897\n",
            "Currect Training Batch:  1898\n",
            "Currect Training Batch:  1899\n",
            "Currect Training Batch:  1900\n",
            "Currect Training Batch:  1901\n",
            "Currect Training Batch:  1902\n",
            "Currect Training Batch:  1903\n",
            "Currect Training Batch:  1904\n",
            "Currect Training Batch:  1905\n",
            "Currect Training Batch:  1906\n",
            "Currect Training Batch:  1907\n",
            "Currect Training Batch:  1908\n",
            "Currect Training Batch:  1909\n",
            "Currect Training Batch:  1910\n",
            "Currect Training Batch:  1911\n",
            "Currect Training Batch:  1912\n",
            "Currect Training Batch:  1913\n",
            "Currect Training Batch:  1914\n",
            "Currect Training Batch:  1915\n",
            "Currect Training Batch:  1916\n",
            "Currect Training Batch:  1917\n",
            "Currect Training Batch:  1918\n",
            "Currect Training Batch:  1919\n",
            "Currect Training Batch:  1920\n",
            "Currect Training Batch:  1921\n",
            "Currect Training Batch:  1922\n",
            "Currect Training Batch:  1923\n",
            "Currect Training Batch:  1924\n",
            "Currect Training Batch:  1925\n",
            "Currect Training Batch:  1926\n",
            "Currect Training Batch:  1927\n",
            "Currect Training Batch:  1928\n",
            "Currect Training Batch:  1929\n",
            "Currect Training Batch:  1930\n",
            "Currect Training Batch:  1931\n",
            "Currect Training Batch:  1932\n",
            "Currect Training Batch:  1933\n",
            "Currect Training Batch:  1934\n",
            "Currect Training Batch:  1935\n",
            "Currect Training Batch:  1936\n",
            "Currect Training Batch:  1937\n",
            "Currect Training Batch:  1938\n",
            "Currect Training Batch:  1939\n",
            "Currect Training Batch:  1940\n",
            "Currect Training Batch:  1941\n",
            "Currect Training Batch:  1942\n",
            "Currect Training Batch:  1943\n",
            "Currect Training Batch:  1944\n",
            "Currect Training Batch:  1945\n",
            "Currect Training Batch:  1946\n",
            "Currect Training Batch:  1947\n",
            "Currect Training Batch:  1948\n",
            "Currect Training Batch:  1949\n",
            "Currect Training Batch:  1950\n",
            "Currect Training Batch:  1951\n",
            "Currect Training Batch:  1952\n",
            "Currect Training Batch:  1953\n",
            "Currect Training Batch:  1954\n",
            "Currect Training Batch:  1955\n",
            "Currect Training Batch:  1956\n",
            "Currect Training Batch:  1957\n",
            "Current Validating Batch:  1\n",
            "Current Validating Batch:  2\n",
            "Current Validating Batch:  3\n",
            "Current Validating Batch:  4\n",
            "Current Validating Batch:  5\n",
            "Current Validating Batch:  6\n",
            "Current Validating Batch:  7\n",
            "Current Validating Batch:  8\n",
            "Current Validating Batch:  9\n",
            "Current Validating Batch:  10\n",
            "Current Validating Batch:  11\n",
            "Current Validating Batch:  12\n",
            "Current Validating Batch:  13\n",
            "Current Validating Batch:  14\n",
            "Current Validating Batch:  15\n",
            "Current Validating Batch:  16\n",
            "Current Validating Batch:  17\n",
            "Current Validating Batch:  18\n",
            "Current Validating Batch:  19\n",
            "Current Validating Batch:  20\n",
            "Current Validating Batch:  21\n",
            "Current Validating Batch:  22\n",
            "Current Validating Batch:  23\n",
            "Current Validating Batch:  24\n",
            "Current Validating Batch:  25\n",
            "Current Validating Batch:  26\n",
            "Current Validating Batch:  27\n",
            "Current Validating Batch:  28\n",
            "Current Validating Batch:  29\n",
            "Epoch: 19 \tTraining Loss: 0.072316 \tValidation Loss: 0.145025 \tAccuracy: 0.001082 \n",
            "Currect Training Batch:  1958\n",
            "Currect Training Batch:  1959\n",
            "Currect Training Batch:  1960\n",
            "Currect Training Batch:  1961\n",
            "Currect Training Batch:  1962\n",
            "Currect Training Batch:  1963\n",
            "Currect Training Batch:  1964\n",
            "Currect Training Batch:  1965\n",
            "Currect Training Batch:  1966\n",
            "Currect Training Batch:  1967\n",
            "Currect Training Batch:  1968\n",
            "Currect Training Batch:  1969\n",
            "Currect Training Batch:  1970\n",
            "Currect Training Batch:  1971\n",
            "Currect Training Batch:  1972\n",
            "Currect Training Batch:  1973\n",
            "Currect Training Batch:  1974\n",
            "Currect Training Batch:  1975\n",
            "Currect Training Batch:  1976\n",
            "Currect Training Batch:  1977\n",
            "Currect Training Batch:  1978\n",
            "Currect Training Batch:  1979\n",
            "Currect Training Batch:  1980\n",
            "Currect Training Batch:  1981\n",
            "Currect Training Batch:  1982\n",
            "Currect Training Batch:  1983\n",
            "Currect Training Batch:  1984\n",
            "Currect Training Batch:  1985\n",
            "Currect Training Batch:  1986\n",
            "Currect Training Batch:  1987\n",
            "Currect Training Batch:  1988\n",
            "Currect Training Batch:  1989\n",
            "Currect Training Batch:  1990\n",
            "Currect Training Batch:  1991\n",
            "Currect Training Batch:  1992\n",
            "Currect Training Batch:  1993\n",
            "Currect Training Batch:  1994\n",
            "Currect Training Batch:  1995\n",
            "Currect Training Batch:  1996\n",
            "Currect Training Batch:  1997\n",
            "Currect Training Batch:  1998\n",
            "Currect Training Batch:  1999\n",
            "Currect Training Batch:  2000\n",
            "Currect Training Batch:  2001\n",
            "Currect Training Batch:  2002\n",
            "Currect Training Batch:  2003\n",
            "Currect Training Batch:  2004\n",
            "Currect Training Batch:  2005\n",
            "Currect Training Batch:  2006\n",
            "Currect Training Batch:  2007\n",
            "Currect Training Batch:  2008\n",
            "Currect Training Batch:  2009\n",
            "Currect Training Batch:  2010\n",
            "Currect Training Batch:  2011\n",
            "Currect Training Batch:  2012\n",
            "Currect Training Batch:  2013\n",
            "Currect Training Batch:  2014\n",
            "Currect Training Batch:  2015\n",
            "Currect Training Batch:  2016\n",
            "Currect Training Batch:  2017\n",
            "Currect Training Batch:  2018\n",
            "Currect Training Batch:  2019\n",
            "Currect Training Batch:  2020\n",
            "Currect Training Batch:  2021\n",
            "Currect Training Batch:  2022\n",
            "Currect Training Batch:  2023\n",
            "Currect Training Batch:  2024\n",
            "Currect Training Batch:  2025\n",
            "Currect Training Batch:  2026\n",
            "Currect Training Batch:  2027\n",
            "Currect Training Batch:  2028\n",
            "Currect Training Batch:  2029\n",
            "Currect Training Batch:  2030\n",
            "Currect Training Batch:  2031\n",
            "Currect Training Batch:  2032\n",
            "Currect Training Batch:  2033\n",
            "Currect Training Batch:  2034\n",
            "Currect Training Batch:  2035\n",
            "Currect Training Batch:  2036\n",
            "Currect Training Batch:  2037\n",
            "Currect Training Batch:  2038\n",
            "Currect Training Batch:  2039\n",
            "Currect Training Batch:  2040\n",
            "Currect Training Batch:  2041\n",
            "Currect Training Batch:  2042\n",
            "Currect Training Batch:  2043\n",
            "Currect Training Batch:  2044\n",
            "Currect Training Batch:  2045\n",
            "Currect Training Batch:  2046\n",
            "Currect Training Batch:  2047\n",
            "Currect Training Batch:  2048\n",
            "Currect Training Batch:  2049\n",
            "Currect Training Batch:  2050\n",
            "Currect Training Batch:  2051\n",
            "Currect Training Batch:  2052\n",
            "Currect Training Batch:  2053\n",
            "Currect Training Batch:  2054\n",
            "Currect Training Batch:  2055\n",
            "Currect Training Batch:  2056\n",
            "Currect Training Batch:  2057\n",
            "Currect Training Batch:  2058\n",
            "Currect Training Batch:  2059\n",
            "Currect Training Batch:  2060\n",
            "Current Validating Batch:  1\n",
            "Current Validating Batch:  2\n",
            "Current Validating Batch:  3\n",
            "Current Validating Batch:  4\n",
            "Current Validating Batch:  5\n",
            "Current Validating Batch:  6\n",
            "Current Validating Batch:  7\n",
            "Current Validating Batch:  8\n",
            "Current Validating Batch:  9\n",
            "Current Validating Batch:  10\n",
            "Current Validating Batch:  11\n",
            "Current Validating Batch:  12\n",
            "Current Validating Batch:  13\n",
            "Current Validating Batch:  14\n",
            "Current Validating Batch:  15\n",
            "Current Validating Batch:  16\n",
            "Current Validating Batch:  17\n",
            "Current Validating Batch:  18\n",
            "Current Validating Batch:  19\n",
            "Current Validating Batch:  20\n",
            "Current Validating Batch:  21\n",
            "Current Validating Batch:  22\n",
            "Current Validating Batch:  23\n",
            "Current Validating Batch:  24\n",
            "Current Validating Batch:  25\n",
            "Current Validating Batch:  26\n",
            "Current Validating Batch:  27\n",
            "Current Validating Batch:  28\n",
            "Current Validating Batch:  29\n",
            "Epoch: 20 \tTraining Loss: 0.072352 \tValidation Loss: 0.145030 \tAccuracy: 0.002165 \n",
            "Currect Training Batch:  2061\n",
            "Currect Training Batch:  2062\n",
            "Currect Training Batch:  2063\n",
            "Currect Training Batch:  2064\n",
            "Currect Training Batch:  2065\n",
            "Currect Training Batch:  2066\n",
            "Currect Training Batch:  2067\n",
            "Currect Training Batch:  2068\n",
            "Currect Training Batch:  2069\n",
            "Currect Training Batch:  2070\n",
            "Currect Training Batch:  2071\n",
            "Currect Training Batch:  2072\n",
            "Currect Training Batch:  2073\n",
            "Currect Training Batch:  2074\n",
            "Currect Training Batch:  2075\n",
            "Currect Training Batch:  2076\n",
            "Currect Training Batch:  2077\n",
            "Currect Training Batch:  2078\n",
            "Currect Training Batch:  2079\n",
            "Currect Training Batch:  2080\n",
            "Currect Training Batch:  2081\n",
            "Currect Training Batch:  2082\n",
            "Currect Training Batch:  2083\n",
            "Currect Training Batch:  2084\n",
            "Currect Training Batch:  2085\n",
            "Currect Training Batch:  2086\n",
            "Currect Training Batch:  2087\n",
            "Currect Training Batch:  2088\n",
            "Currect Training Batch:  2089\n",
            "Currect Training Batch:  2090\n",
            "Currect Training Batch:  2091\n",
            "Currect Training Batch:  2092\n",
            "Currect Training Batch:  2093\n",
            "Currect Training Batch:  2094\n",
            "Currect Training Batch:  2095\n",
            "Currect Training Batch:  2096\n",
            "Currect Training Batch:  2097\n",
            "Currect Training Batch:  2098\n",
            "Currect Training Batch:  2099\n",
            "Currect Training Batch:  2100\n",
            "Currect Training Batch:  2101\n",
            "Currect Training Batch:  2102\n",
            "Currect Training Batch:  2103\n",
            "Currect Training Batch:  2104\n",
            "Currect Training Batch:  2105\n",
            "Currect Training Batch:  2106\n",
            "Currect Training Batch:  2107\n",
            "Currect Training Batch:  2108\n",
            "Currect Training Batch:  2109\n",
            "Currect Training Batch:  2110\n",
            "Currect Training Batch:  2111\n",
            "Currect Training Batch:  2112\n",
            "Currect Training Batch:  2113\n",
            "Currect Training Batch:  2114\n",
            "Currect Training Batch:  2115\n",
            "Currect Training Batch:  2116\n",
            "Currect Training Batch:  2117\n",
            "Currect Training Batch:  2118\n",
            "Currect Training Batch:  2119\n",
            "Currect Training Batch:  2120\n",
            "Currect Training Batch:  2121\n",
            "Currect Training Batch:  2122\n",
            "Currect Training Batch:  2123\n",
            "Currect Training Batch:  2124\n",
            "Currect Training Batch:  2125\n",
            "Currect Training Batch:  2126\n",
            "Currect Training Batch:  2127\n",
            "Currect Training Batch:  2128\n",
            "Currect Training Batch:  2129\n",
            "Currect Training Batch:  2130\n",
            "Currect Training Batch:  2131\n",
            "Currect Training Batch:  2132\n",
            "Currect Training Batch:  2133\n",
            "Currect Training Batch:  2134\n",
            "Currect Training Batch:  2135\n",
            "Currect Training Batch:  2136\n",
            "Currect Training Batch:  2137\n",
            "Currect Training Batch:  2138\n",
            "Currect Training Batch:  2139\n",
            "Currect Training Batch:  2140\n",
            "Currect Training Batch:  2141\n",
            "Currect Training Batch:  2142\n",
            "Currect Training Batch:  2143\n",
            "Currect Training Batch:  2144\n",
            "Currect Training Batch:  2145\n",
            "Currect Training Batch:  2146\n",
            "Currect Training Batch:  2147\n",
            "Currect Training Batch:  2148\n",
            "Currect Training Batch:  2149\n",
            "Currect Training Batch:  2150\n",
            "Currect Training Batch:  2151\n",
            "Currect Training Batch:  2152\n",
            "Currect Training Batch:  2153\n",
            "Currect Training Batch:  2154\n",
            "Currect Training Batch:  2155\n",
            "Currect Training Batch:  2156\n",
            "Currect Training Batch:  2157\n",
            "Currect Training Batch:  2158\n",
            "Currect Training Batch:  2159\n",
            "Currect Training Batch:  2160\n",
            "Currect Training Batch:  2161\n",
            "Currect Training Batch:  2162\n",
            "Currect Training Batch:  2163\n",
            "Current Validating Batch:  1\n",
            "Current Validating Batch:  2\n",
            "Current Validating Batch:  3\n",
            "Current Validating Batch:  4\n",
            "Current Validating Batch:  5\n",
            "Current Validating Batch:  6\n",
            "Current Validating Batch:  7\n",
            "Current Validating Batch:  8\n",
            "Current Validating Batch:  9\n",
            "Current Validating Batch:  10\n",
            "Current Validating Batch:  11\n",
            "Current Validating Batch:  12\n",
            "Current Validating Batch:  13\n",
            "Current Validating Batch:  14\n",
            "Current Validating Batch:  15\n",
            "Current Validating Batch:  16\n",
            "Current Validating Batch:  17\n",
            "Current Validating Batch:  18\n",
            "Current Validating Batch:  19\n",
            "Current Validating Batch:  20\n",
            "Current Validating Batch:  21\n",
            "Current Validating Batch:  22\n",
            "Current Validating Batch:  23\n",
            "Current Validating Batch:  24\n",
            "Current Validating Batch:  25\n",
            "Current Validating Batch:  26\n",
            "Current Validating Batch:  27\n",
            "Current Validating Batch:  28\n",
            "Current Validating Batch:  29\n",
            "Epoch: 21 \tTraining Loss: 0.072379 \tValidation Loss: 0.145021 \tAccuracy: 0.002165 \n",
            "Currect Training Batch:  2164\n",
            "Currect Training Batch:  2165\n",
            "Currect Training Batch:  2166\n",
            "Currect Training Batch:  2167\n",
            "Currect Training Batch:  2168\n",
            "Currect Training Batch:  2169\n",
            "Currect Training Batch:  2170\n",
            "Currect Training Batch:  2171\n",
            "Currect Training Batch:  2172\n",
            "Currect Training Batch:  2173\n",
            "Currect Training Batch:  2174\n",
            "Currect Training Batch:  2175\n",
            "Currect Training Batch:  2176\n",
            "Currect Training Batch:  2177\n",
            "Currect Training Batch:  2178\n",
            "Currect Training Batch:  2179\n",
            "Currect Training Batch:  2180\n",
            "Currect Training Batch:  2181\n",
            "Currect Training Batch:  2182\n",
            "Currect Training Batch:  2183\n",
            "Currect Training Batch:  2184\n",
            "Currect Training Batch:  2185\n",
            "Currect Training Batch:  2186\n",
            "Currect Training Batch:  2187\n",
            "Currect Training Batch:  2188\n",
            "Currect Training Batch:  2189\n",
            "Currect Training Batch:  2190\n",
            "Currect Training Batch:  2191\n",
            "Currect Training Batch:  2192\n",
            "Currect Training Batch:  2193\n",
            "Currect Training Batch:  2194\n",
            "Currect Training Batch:  2195\n",
            "Currect Training Batch:  2196\n",
            "Currect Training Batch:  2197\n",
            "Currect Training Batch:  2198\n",
            "Currect Training Batch:  2199\n",
            "Currect Training Batch:  2200\n",
            "Currect Training Batch:  2201\n",
            "Currect Training Batch:  2202\n",
            "Currect Training Batch:  2203\n",
            "Currect Training Batch:  2204\n",
            "Currect Training Batch:  2205\n",
            "Currect Training Batch:  2206\n",
            "Currect Training Batch:  2207\n",
            "Currect Training Batch:  2208\n",
            "Currect Training Batch:  2209\n",
            "Currect Training Batch:  2210\n",
            "Currect Training Batch:  2211\n",
            "Currect Training Batch:  2212\n",
            "Currect Training Batch:  2213\n",
            "Currect Training Batch:  2214\n",
            "Currect Training Batch:  2215\n",
            "Currect Training Batch:  2216\n",
            "Currect Training Batch:  2217\n",
            "Currect Training Batch:  2218\n",
            "Currect Training Batch:  2219\n",
            "Currect Training Batch:  2220\n",
            "Currect Training Batch:  2221\n",
            "Currect Training Batch:  2222\n",
            "Currect Training Batch:  2223\n",
            "Currect Training Batch:  2224\n",
            "Currect Training Batch:  2225\n",
            "Currect Training Batch:  2226\n",
            "Currect Training Batch:  2227\n",
            "Currect Training Batch:  2228\n",
            "Currect Training Batch:  2229\n",
            "Currect Training Batch:  2230\n",
            "Currect Training Batch:  2231\n",
            "Currect Training Batch:  2232\n",
            "Currect Training Batch:  2233\n",
            "Currect Training Batch:  2234\n",
            "Currect Training Batch:  2235\n",
            "Currect Training Batch:  2236\n",
            "Currect Training Batch:  2237\n",
            "Currect Training Batch:  2238\n",
            "Currect Training Batch:  2239\n",
            "Currect Training Batch:  2240\n",
            "Currect Training Batch:  2241\n",
            "Currect Training Batch:  2242\n",
            "Currect Training Batch:  2243\n",
            "Currect Training Batch:  2244\n",
            "Currect Training Batch:  2245\n",
            "Currect Training Batch:  2246\n",
            "Currect Training Batch:  2247\n",
            "Currect Training Batch:  2248\n",
            "Currect Training Batch:  2249\n",
            "Currect Training Batch:  2250\n",
            "Currect Training Batch:  2251\n",
            "Currect Training Batch:  2252\n",
            "Currect Training Batch:  2253\n",
            "Currect Training Batch:  2254\n",
            "Currect Training Batch:  2255\n",
            "Currect Training Batch:  2256\n",
            "Currect Training Batch:  2257\n",
            "Currect Training Batch:  2258\n",
            "Currect Training Batch:  2259\n",
            "Currect Training Batch:  2260\n",
            "Currect Training Batch:  2261\n",
            "Currect Training Batch:  2262\n",
            "Currect Training Batch:  2263\n",
            "Currect Training Batch:  2264\n",
            "Currect Training Batch:  2265\n",
            "Currect Training Batch:  2266\n",
            "Current Validating Batch:  1\n",
            "Current Validating Batch:  2\n",
            "Current Validating Batch:  3\n",
            "Current Validating Batch:  4\n",
            "Current Validating Batch:  5\n",
            "Current Validating Batch:  6\n",
            "Current Validating Batch:  7\n",
            "Current Validating Batch:  8\n",
            "Current Validating Batch:  9\n",
            "Current Validating Batch:  10\n",
            "Current Validating Batch:  11\n",
            "Current Validating Batch:  12\n",
            "Current Validating Batch:  13\n",
            "Current Validating Batch:  14\n",
            "Current Validating Batch:  15\n",
            "Current Validating Batch:  16\n",
            "Current Validating Batch:  17\n",
            "Current Validating Batch:  18\n",
            "Current Validating Batch:  19\n",
            "Current Validating Batch:  20\n",
            "Current Validating Batch:  21\n",
            "Current Validating Batch:  22\n",
            "Current Validating Batch:  23\n",
            "Current Validating Batch:  24\n",
            "Current Validating Batch:  25\n",
            "Current Validating Batch:  26\n",
            "Current Validating Batch:  27\n",
            "Current Validating Batch:  28\n",
            "Current Validating Batch:  29\n",
            "Epoch: 22 \tTraining Loss: 0.072335 \tValidation Loss: 0.145019 \tAccuracy: 0.001082 \n",
            "Currect Training Batch:  2267\n",
            "Currect Training Batch:  2268\n",
            "Currect Training Batch:  2269\n",
            "Currect Training Batch:  2270\n",
            "Currect Training Batch:  2271\n",
            "Currect Training Batch:  2272\n",
            "Currect Training Batch:  2273\n",
            "Currect Training Batch:  2274\n",
            "Currect Training Batch:  2275\n",
            "Currect Training Batch:  2276\n",
            "Currect Training Batch:  2277\n",
            "Currect Training Batch:  2278\n",
            "Currect Training Batch:  2279\n",
            "Currect Training Batch:  2280\n",
            "Currect Training Batch:  2281\n",
            "Currect Training Batch:  2282\n",
            "Currect Training Batch:  2283\n",
            "Currect Training Batch:  2284\n",
            "Currect Training Batch:  2285\n",
            "Currect Training Batch:  2286\n",
            "Currect Training Batch:  2287\n",
            "Currect Training Batch:  2288\n",
            "Currect Training Batch:  2289\n",
            "Currect Training Batch:  2290\n",
            "Currect Training Batch:  2291\n",
            "Currect Training Batch:  2292\n",
            "Currect Training Batch:  2293\n",
            "Currect Training Batch:  2294\n",
            "Currect Training Batch:  2295\n",
            "Currect Training Batch:  2296\n",
            "Currect Training Batch:  2297\n",
            "Currect Training Batch:  2298\n",
            "Currect Training Batch:  2299\n",
            "Currect Training Batch:  2300\n",
            "Currect Training Batch:  2301\n",
            "Currect Training Batch:  2302\n",
            "Currect Training Batch:  2303\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-617c3f18b4c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m       \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m######################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "BVDXg36OBEb6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def savemodel(model,model_save_name,trainset):\n",
        " model.class_to_idx =trainset.class_to_idx\n",
        " #path=F\"/content/gdrive/myDrive/{model_save_name}\"\n",
        " torch.save({'arch': 'resnet152',\n",
        "           'state_dict': model.state_dict(),\n",
        "           'class_to_idx': model.class_to_idx},\n",
        "           model_save_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WNKOkeJTmxtr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(model, dataloader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_predictions_vector = None\n",
        "        counter=0\n",
        "        for index, (inputs, labels) in enumerate(dataloader):\n",
        "            inputs.to(torch.device(\"cpu\"))\n",
        "            labels.to(torch.device(\"cpu\"))\n",
        "            counter+=1\n",
        "            print(counter)\n",
        "            outputs = model(inputs)\n",
        "            _, y_hat = outputs.max(1)\n",
        "            current_batch_predictions_vector = (y_hat==labels.data)\n",
        "            if index == 0:\n",
        "                total_predictions_vector = current_batch_predictions_vector\n",
        "            else:\n",
        "              total_predictions_vector = torch.cat(( current_batch_predictions_vector, total_predictions_vector), 0)\n",
        "\n",
        "        total_predictions_vector = total_predictions_vector.cpu()\n",
        "        valid_items = total_predictions_vector.sum().numpy()\n",
        "        total_items = list(total_predictions_vector.size())[0]\n",
        "        accuracy = valid_items/total_items\n",
        "        return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YEE4arVgmyQj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filepath = \"drive/My Drive/AI/flower_102_model.pt\"\n",
        "state_dict = torch.load(filepath, map_location='cpu')\n",
        "model.load_state_dict(state_dict, strict=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nf-9Tg5snm8z",
        "colab_type": "code",
        "outputId": "378ac462-94a1-4855-c1ea-1f399eea0b79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1014
        }
      },
      "cell_type": "code",
      "source": [
        "calculate_accuracy(model, validLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-80dec2329841>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcalculate_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-63-da0aba1f0cf5>\u001b[0m in \u001b[0;36mcalculate_accuracy\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mcounter\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mcurrent_batch_predictions_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/densenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected object of type torch.FloatTensor but found type torch.cuda.FloatTensor for argument #2 'weight'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "zqU4HJ5dn8S0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}